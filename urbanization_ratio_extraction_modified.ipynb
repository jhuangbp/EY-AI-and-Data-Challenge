{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e40a2b97",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Urbanization Ratio Extraction for Water Quality Datasets\n\nThis notebook computes urbanization ratios around sampling locations from the provided water quality training dataset and submission template.\n\nThe urbanization ratio is calculated as the fraction of pixels classified as Urban areas (class 190) in the ESA CCI Land Cover (annual 300 m resolution) dataset within buffer zones (e.g., 500 m and 1 km) around each sampling point.\n\n**Data sources:**\n\n- `water_quality_training_dataset.csv`: training data with latitude and longitude\n- `submission_template.csv`: validation set locations\n\n**Outputs:**\n\nThis notebook reads both CSVs, extracts unique sampling coordinates, computes urbanization ratios for each year from 2011 to 2015 and buffer distances (500 m and 1 km), and merges the results back to the training and submission datasets.\n\nNote: Running the ratio computation across all points may take significant time since it downloads and processes raster tiles from the Microsoft Planetary Computer. A progress bar is displayed during computation."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f33406f",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# If running in a fresh environment, uncomment the following line to install required packages\n!pip install pystac-client planetary-computer geopandas shapely numpy pandas rioxarray rasterio tqdm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26119bef",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nfrom shapely.geometry import mapping, box\nimport requests\nimport time\nfrom tqdm import tqdm\n\nimport planetary_computer\n\n# Raster handling\nimport rioxarray as rxr\nimport rasterio\n\n# For functional operations\nfrom functools import reduce\nfrom functools import lru_cache"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f1a00e",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# STAC search helper with pagination\nSTAC_SEARCH_URL = 'https://planetarycomputer.microsoft.com/api/stac/v1/search'\n\ndef search_items_stac(collection_id, bbox, start_date, end_date,\n                      batch_size=200, sleep_seconds=0.2, max_retries=5):\n    \"\"\"Perform a STAC search and return all items within the specified date range and bounding box.\"\"\"\n    payload = {\n        'collections': [collection_id],\n        'bbox': bbox,\n        'datetime': f'{start_date}/{end_date}',\n        'limit': batch_size\n    }\n    items = []\n    session = requests.Session()\n\n    # 用於指數退避的初始等待\n    backoff = sleep_seconds\n\n    # 嘗試搜尋及重試\n    for attempt in range(max_retries):\n        try:\n            response = session.post(STAC_SEARCH_URL, json=payload, timeout=30)\n            response.raise_for_status()\n            data = response.json()\n            batch_items = data.get('features', []) or []\n            items.extend(batch_items)\n\n            # Helper to find the next link\n            def get_next_link(obj):\n                for link in obj.get('links', []):\n                    if link.get('rel') == 'next':\n                        return link.get('href')\n                return None\n\n            next_link = get_next_link(data)\n            # 依序取得下一頁資源\n            while next_link:\n                time.sleep(sleep_seconds)\n                resp = session.get(next_link, timeout=30)\n                resp.raise_for_status()\n                data = resp.json()\n                batch_items = data.get('features', []) or []\n                items.extend(batch_items)\n                next_link = get_next_link(data)\n\n            return items\n\n        except requests.exceptions.HTTPError as e:\n            # 需要重試的狀態碼\n            if response.status_code in [429, 500, 502, 503, 504] and attempt < max_retries - 1:\n                # 指數退避等待\n                time.sleep(backoff)\n                backoff *= 2\n                continue\n            else:\n                raise\n\n    return items  # 若多次重試後仍失敗，回傳現有結果或空"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06ea751c",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import random\n\ndef open_raster_with_retry(href, tries=5, base_sleep=0.5):\n    '''Open a raster dataset with retries to handle transient HTTP errors.'''\n    last_err = None\n    for t in range(tries):\n        try:\n            return rxr.open_rasterio(href, masked=True)\n        except rasterio.errors.RasterioIOError as e:\n            last_err = e\n            time.sleep(base_sleep * (2 ** t) + random.random() * 0.2)\n    raise last_err\n\n@lru_cache(maxsize=16)\ndef search_stac_cached(collection_id, bbox, start_date, end_date):\n    \"\"\"Wrapper for lru_cache to avoid repeated searches over the same range.\"\"\"\n    return tuple(search_items_stac(collection_id, bbox, start_date, end_date))\n\nfrom shapely.geometry import box\n\ndef pick_item_for_buffer(items, buffer_geom, year):\n    \"\"\"Pick a STAC item that contains (or at least intersects) the buffer bbox.\"\"\"\n    minx, miny, maxx, maxy = buffer_geom.bounds\n    buf_box = box(minx, miny, maxx, maxy)\n    # Filter by year substring in ID if available\n    year_items = [it for it in items if f\"-{year}-\" in it.get(\"id\", \"\")]\n    candidates = year_items if year_items else list(items)\n    contains = []\n    intersects = []\n    for it in candidates:\n        ib = it.get(\"bbox\")\n        if not ib:\n            continue\n        it_box = box(*ib)\n        if it_box.contains(buf_box):\n            contains.append((it_box.area, it))\n        elif it_box.intersects(buf_box):\n            intersects.append((it_box.area, it))\n    if contains:\n        contains.sort(key=lambda x: x[0])\n        return contains[0][1]\n    if intersects:\n        intersects.sort(key=lambda x: x[0])\n        return intersects[0][1]\n    return candidates[0] if candidates else None\n\ndef compute_builtup_ratio(\n    gdf, buffer_distance, year, landcover_collection=\"esa-cci-lc\",\n    batch_size=200, sleep_seconds=0.2):\n    \"\"\"Compute built-up ratio for each geometry in gdf at the given buffer distance and year.\"\"\"\n    results = []\n    start_date = f\"{year}-01-01\"\n    end_date = f\"{year}-12-31\"\n    # calculate unified bounding box for all point buffers\n    buffers_3857 = gdf.to_crs(\"EPSG:3857\").geometry.buffer(buffer_distance)\n    buffers_4326 = gpd.GeoSeries(buffers_3857, crs=\"EPSG:3857\").to_crs(\"EPSG:4326\")\n    union_minx, union_miny, union_maxx, union_maxy = buffers_4326.total_bounds\n    union_bbox = [union_minx, union_miny, union_maxx, union_maxy]\n    # search once per year for all items in bounding box\n    items = search_stac_cached(landcover_collection, tuple(union_bbox), start_date, end_date)\n    items = list(items)\n    for idx, buffer_geom in tqdm(enumerate(buffers_4326), total=len(buffers_4326),\n                                 desc=f\"Buffer {buffer_distance}m, Year {year}\"):\n        selected_item = pick_item_for_buffer(items, buffer_geom, year)\n        ratio = np.nan\n        if selected_item is not None:\n            asset_key = \"map\" if \"map\" in selected_item[\"assets\"] else list(selected_item[\"assets\"].keys())[0]\n            raw_href = selected_item[\"assets\"][asset_key][\"href\"]\n            signed_href = planetary_computer.sign(raw_href)\n            try:\n                arr = open_raster_with_retry(signed_href)\n                # Clip raster to the single buffer\n                masked = arr.rio.clip([mapping(buffer_geom)], drop=True, invert=False)\n                data = masked.values\n                valid = data[~np.isnan(data)]\n                built_up = (valid == 190).sum()\n                ratio = built_up / len(valid) if len(valid) > 0 else np.nan\n            except Exception:\n                ratio = np.nan\n        results.append({\"geometry_index\": idx,\n                        f\"built_up_ratio_{int(buffer_distance)}m_{year}\": ratio})\n    return pd.DataFrame(results)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca7f274",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Paths to datasets (adjust if necessary)\ntrain_path = 'water_quality_training_dataset.csv'\nsubmission_path = 'submission_template.csv'\n\n# Read datasets\ntrain_df = pd.read_csv(train_path)\nsubmission_df = pd.read_csv(submission_path)\n\nprint('Training data shape:', train_df.shape)\nprint('Submission data shape:', submission_df.shape)\n\n# Extract unique coordinate pairs\ntrain_coords = train_df[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True)\nsubmission_coords = submission_df[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True)\n\nall_coords = pd.concat([train_coords, submission_coords]).drop_duplicates().reset_index(drop=True)\n\n# Create GeoDataFrame\npoints_gdf = gpd.GeoDataFrame(all_coords, geometry=gpd.points_from_xy(all_coords['Longitude'], all_coords['Latitude']), crs='EPSG:4326')\n\nprint('Total unique points:', len(points_gdf))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5cc74df",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Define buffer distances and years\nbuffer_distances = [1000,5000]\nyears = [2011, 2012, 2013, 2014, 2015]\n\nratio_frames = []\nfor d in buffer_distances:\n    year_frames = []\n    for yr in years:\n        df_ratio = compute_builtup_ratio(points_gdf, buffer_distance=d, year=yr, landcover_collection='esa-cci-lc')\n        year_frames.append(df_ratio)\n    merged_years = reduce(lambda left, right: left.merge(right, on='geometry_index'), year_frames)\n    ratio_frames.append(merged_years)\n\n# Merge buffer distances\nfinal_ratio = reduce(lambda left, right: left.merge(right, on='geometry_index'), ratio_frames)\n\n# Add coordinates back\nfinal_ratio = pd.concat([points_gdf[['Latitude', 'Longitude']].reset_index(drop=True), final_ratio.drop(columns=['geometry_index'])], axis=1)\n\n# Merge back to training and submission\ntrain_with_ratio = train_df.merge(final_ratio, on=['Latitude', 'Longitude'], how='left')\nsubmission_with_ratio = submission_df.merge(final_ratio, on=['Latitude', 'Longitude'], how='left')\n\n# Display results\nprint('Training data with ratios:')\ndisplay(train_with_ratio.head())\nprint('Submission data with ratios:')\ndisplay(submission_with_ratio.head())"
    },
    {
      "id": "9f648e13-31bf-48b5-bef3-607215304391",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "submission_with_ratio",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3ad32e6e-9fa6-4a1b-a8bc-af54594f4115",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "train_with_ratio",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2f9da44e-deb3-4f4d-939b-527c8de8ecac",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "train_with_ratio.to_csv(\"train_with_ratio.csv\", index=False)\nsubmission_with_ratio.to_csv(\"submission_with_ratio.csv\", index=False)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fee14a4c-686e-4f98-bda0-b0d71d31eae4",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8a9005a5-bf87-4ca1-a1e3-167df55678e8",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "train_with_ratio.to_csv(\"/tmp/urbanization_train.csv\",index = False)\n\nsession.sql(\"\"\"\n    PUT file:///tmp/urbanization_train.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge-2\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ab8460f4-7ea0-4497-8c8e-fdaf61f983ad",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "train_with_ratio.to_csv(\"/tmp/urbanization_val.csv\",index = False)\n\nsession.sql(\"\"\"\n    PUT file:///tmp/urbanization_val.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge-2\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}