{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b92d672a-a1ff-4806-9dce-5f8c9a4f051b",
      "metadata": {
        "name": "cell1",
        "codeCollapsed": true
      },
      "source": "# Water Quality Prediction: XGBoost Regression Notebook\n\nThis notebook is based on `BENCHMARK_Jin.ipynb` but replaces **Random Forest** with **XGBoost Regression** to improve prediction accuracy."
    },
    {
      "cell_type": "markdown",
      "id": "173e8dca-8e21-478c-b9d8-8162214025ef",
      "metadata": {
        "name": "cell2",
        "codeCollapsed": true
      },
      "source": "## Challenge Overview"
    },
    {
      "cell_type": "markdown",
      "id": "0d38782b-973e-4f63-83b3-3e556abae629",
      "metadata": {
        "collapsed": false,
        "name": "cell3",
        "codeCollapsed": true
      },
      "source": "Welcome to the EY AI & Data Challenge 2026!  \nThe objective of this challenge is to build a robust **machine learning model** capable of predicting water quality across various river locations in South Africa. In addition to accurate predictions, the model should also identify and emphasize the key factors that significantly influence water quality.\n\nParticipants will be provided with a dataset containing three water quality parameters — **Total Alkalinity**, **Electrical Conductance**, and **Dissolved Reactive Phosphorus** — collected between 2011 and 2015 from approximately 200 river locations across South Africa. Each data point includes the geographic coordinates (latitude and longitude) of the sampling site, the date of collection, and the corresponding water quality measurements.\n\nUsing this dataset, participants are expected to build a machine learning model to predict water quality parameters for a separate validation dataset, which includes locations from different regions not present in the training data. The challenge also encourages participants to explore feature importance and provide insights into the factors most strongly associated with variations in water quality.\n\nThis challenge is designed for participants with varying levels of experience in data science, remote sensing, and environmental analytics. It offers a valuable opportunity to apply machine learning techniques to real-world environmental data and contribute to advancing water quality monitoring using artificial intelligence."
    },
    {
      "cell_type": "markdown",
      "id": "57447c9d-ceca-4a26-8066-2dca61e0e224",
      "metadata": {
        "collapsed": false,
        "name": "cell6",
        "codeCollapsed": true
      },
      "source": "## Load In Dependencies"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7871c0-d5f3-45da-a289-3d19db67bf15",
      "metadata": {
        "language": "python",
        "name": "cell58"
      },
      "outputs": [],
      "source": "!pip install uv\n!uv pip install  -r requirements.txt "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b53ef74f-52ba-4c63-b412-2f4432408d04",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell8"
      },
      "outputs": [],
      "source": "import snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# Multi-dimensional arrays and datasets (e.g., NetCDF, Zarr)\nimport xarray as xr\n\n# Geospatial raster data handling with CRS support\nimport rioxarray as rxr\n\n# Raster operations and spatial windowing\nimport rasterio\nfrom rasterio.windows import Window\n\n# Feature preprocessing and data splitting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom scipy.spatial import cKDTree\n\n# Machine Learning - XGBoost\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n# Planetary Computer tools for STAC API access and authentication\nimport pystac_client\nimport planetary_computer as pc\nfrom odc.stac import stac_load\nfrom pystac.extensions.eo import EOExtension as eo\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os "
    },
    {
      "cell_type": "markdown",
      "id": "fdfcbb20-8dff-401a-9f55-ed5d08eb6b60",
      "metadata": {
        "collapsed": false,
        "name": "cell9",
        "codeCollapsed": true
      },
      "source": "## Response Variable"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892acc46-4840-489a-8c69-ecf4123d2a31",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "Water_Quality_df = pd.read_csv(\"water_quality_training_dataset.csv\")\ndisplay(Water_Quality_df.head(5))"
    },
    {
      "cell_type": "markdown",
      "id": "1ac87b43-019d-4aa4-bb4b-62b40cc6cda2",
      "metadata": {
        "collapsed": false,
        "name": "cell12",
        "codeCollapsed": true
      },
      "source": "## Predictor Variables"
    },
    {
      "cell_type": "markdown",
      "id": "83a0c7e8-9471-4a09-9ac4-6a5b5f0703bd",
      "metadata": {
        "collapsed": false,
        "name": "cell15",
        "codeCollapsed": true
      },
      "source": "### Loading Pre-Extracted Landsat Data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c250c49-d6ed-42f7-ad90-b2f619de0027",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "landsat_train_features = pd.read_csv(\"landsat_features_training_200m.csv\")\ndisplay(landsat_train_features.head(5))"
    },
    {
      "cell_type": "markdown",
      "id": "8ce0ed5d-32c0-48f9-863c-ef84bad110d2",
      "metadata": {
        "collapsed": false,
        "name": "cell18",
        "codeCollapsed": true
      },
      "source": "### Loading Pre-Extracted TerraClimate Data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1862bd48-8134-477a-bcf5-314ac04b2048",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "Terraclimate_df = pd.read_csv(\"terraclimate_features_training_full.csv\")\ndisplay(Terraclimate_df.head(50))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91288654-b9dc-4405-a9dc-54b00ae8c371",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "urbanization_df = pd.read_csv(\"urbanization_train.csv\")\n\ncat_cols = [\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"]\n\nurbanization_df = urbanization_df.drop(columns=cat_cols)\n\ndisplay(urbanization_df.head(5))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11f85eb5-21fd-4c06-8c89-afe23e305eab",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "urbanization_df.dtypes"
    },
    {
      "cell_type": "markdown",
      "id": "475de826-a08c-4ea1-bbfc-5e9a3240ab22",
      "metadata": {
        "collapsed": false,
        "name": "cell20",
        "codeCollapsed": true
      },
      "source": "## Joining the Predictor Variables and Response Variables"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b42801e-e7b9-4b17-b4e4-f65fd971decc",
      "metadata": {
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": "# Combine two datasets vertically (along columns) using pandas concat function.\ndef combine_three_datasets(dataset1,dataset2,dataset3):\n    '''\n    Returns a  vertically concatenated dataset.\n    Attributes:\n    dataset1 - Dataset 1 to be combined \n    dataset2 - Dataset 2 to be combined\n    '''\n    \n    data = pd.concat([dataset1,dataset2,dataset3], axis=1)\n    data = data.loc[:, ~data.columns.duplicated()]\n    return data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75dffde-a8e3-4a6e-bbab-bf0d8aac7bfe",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell22"
      },
      "outputs": [],
      "source": "# Combining ground data and final data into a single dataset.\nwq_data = combine_three_datasets(Water_Quality_df, landsat_train_features, Terraclimate_df)\ndisplay(wq_data.head(5))"
    },
    {
      "cell_type": "markdown",
      "id": "c4573656-100c-49e7-b3bb-536e0a6290ac",
      "metadata": {
        "collapsed": false,
        "name": "cell23",
        "codeCollapsed": true
      },
      "source": "### Handling Missing Values"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de7372e4-26c6-4bd8-9d7a-39bc35b01a14",
      "metadata": {
        "language": "python",
        "name": "cell24"
      },
      "outputs": [],
      "source": "wq_data = wq_data.fillna(wq_data.median(numeric_only=True))\nwq_data.isna().sum()"
    },
    {
      "cell_type": "markdown",
      "id": "560a997d-ec88-4ae0-b540-44cd3e5f6cf2",
      "metadata": {
        "name": "cell25",
        "codeCollapsed": true
      },
      "source": "## Model Building (XGBoost Regression)"
    },
    {
      "cell_type": "markdown",
      "id": "8e9f7c65-b3a1-405b-b912-cef4e5157bce",
      "metadata": {
        "collapsed": false,
        "name": "cell26",
        "codeCollapsed": true
      },
      "source": "### XGBoost vs Random Forest\n\nXGBoost (eXtreme Gradient Boosting) is a gradient boosting algorithm that builds trees **sequentially**, where each new tree corrects the errors of the previous ensemble. Key advantages over Random Forest:\n\n- **Boosting** learns from residuals, often achieving better accuracy\n- **Regularization** (L1/L2) reduces overfitting\n- **Early stopping** automatically finds the optimal number of trees\n- **Built-in handling** of missing values\n\nWe keep the same features from Landsat and TerraClimate as the original benchmark."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e68f6e-1679-467f-803c-e819d654bbab",
      "metadata": {
        "language": "python",
        "name": "cell31"
      },
      "outputs": [],
      "source": "def scale_data(X_train, X_test):\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled, scaler\n\ndef get_xgb_model():\n    \"\"\"\n    建立 XGBoost 模型，使用更保守的超參數來減少 overfitting\n    \"\"\"\n    return XGBRegressor(\n        n_estimators=1000,\n        max_depth=4,                # 降低深度：4 比 6 更不容易過擬合\n        learning_rate=0.03,         # 降低學習率：更慢但更穩定\n        subsample=0.7,              # 每棵樹只用 70% 樣本\n        colsample_bytree=0.7,       # 每棵樹只用 70% 特徵\n        colsample_bylevel=0.7,      # 每層分裂也限制特徵\n        reg_alpha=1.0,              # 加強 L1 正則化（原 0.1）\n        reg_lambda=5.0,             # 加強 L2 正則化（原 1.0）\n        min_child_weight=10,        # 提高門檻（原 5），避免學到噪音\n        gamma=0.3,                  # 提高分裂門檻（原 0.1）\n        random_state=42,\n        n_jobs=-1,\n        verbosity=0,\n        early_stopping_rounds=50\n    )\n\ndef evaluate_model(model, X_scaled, y_true, dataset_name=\"Test\"):\n    y_pred = model.predict(X_scaled)\n    r2 = r2_score(y_true, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    print(f\"\\n{dataset_name} Evaluation:\")\n    print(f\"R²: {r2:.3f}\")\n    print(f\"RMSE: {rmse:.3f}\")\n    return y_pred, r2, rmse"
    },
    {
      "cell_type": "markdown",
      "id": "39cec372-283e-404a-a989-714969c53b0d",
      "metadata": {
        "collapsed": false,
        "name": "cell32",
        "codeCollapsed": true
      },
      "source": "## Model Workflow — XGBoost with 5-Fold Cross-Validation\n\nWe use **5-fold CV** to get a robust, unbiased estimate of model performance. This avoids the problem of using the test set for early stopping (which can leak information). The workflow:\n\n1. **CV phase**: 5-fold CV with early stopping on the fold's validation set → reports mean ± std R² and RMSE\n2. **Final model**: Retrain on **all** training data (using best `n_estimators` from CV) → used for submission predictions"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40808f99-8014-4746-91ae-5b64c61e04a7",
      "metadata": {
        "language": "python",
        "name": "cell33"
      },
      "outputs": [],
      "source": "def run_pipeline_cv(X, y, param_name=\"Parameter\", n_folds=5):\n    print(f\"\\n{'='*60}\")\n    print(f\"Training XGBoost Model for {param_name} ({n_folds}-Fold CV)\")\n    print(f\"{'='*60}\")\n    \n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    cv_r2_train = []\n    cv_r2_test = []\n    cv_rmse_train = []\n    cv_rmse_test = []\n    best_iterations = []\n    \n    scaler = StandardScaler()\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # Scale\n        X_train_scaled = scaler.fit_transform(X_train_fold)\n        X_val_scaled = scaler.transform(X_val_fold)\n        \n        # Train with early stopping\n        model = get_xgb_model()\n        model.fit(\n            X_train_scaled, y_train_fold,\n            eval_set=[(X_val_scaled, y_val_fold)],\n            verbose=False\n        )\n        \n        try:\n            best_iter = model.best_iteration\n        except AttributeError:\n            best_iter = model.n_estimators\n        best_iterations.append(best_iter)\n        \n        # Evaluate train\n        y_train_pred = model.predict(X_train_scaled)\n        r2_tr = r2_score(y_train_fold, y_train_pred)\n        rmse_tr = np.sqrt(mean_squared_error(y_train_fold, y_train_pred))\n        \n        # Evaluate val\n        y_val_pred = model.predict(X_val_scaled)\n        r2_val = r2_score(y_val_fold, y_val_pred)\n        rmse_val = np.sqrt(mean_squared_error(y_val_fold, y_val_pred))\n        \n        cv_r2_train.append(r2_tr)\n        cv_r2_test.append(r2_val)\n        cv_rmse_train.append(rmse_tr)\n        cv_rmse_test.append(rmse_val)\n        \n        print(f\"  Fold {fold}: Train R²={r2_tr:.3f}, Val R²={r2_val:.3f}, \"\n              f\"Best iter={best_iter}\")\n    \n    print(f\"\\n  CV Summary:\")\n    print(f\"  Train R²: {np.mean(cv_r2_train):.3f} ± {np.std(cv_r2_train):.3f}\")\n    print(f\"  Val   R²: {np.mean(cv_r2_test):.3f} ± {np.std(cv_r2_test):.3f}\")\n    print(f\"  Val RMSE: {np.mean(cv_rmse_test):.3f} ± {np.std(cv_rmse_test):.3f}\")\n    print(f\"  Avg best iteration: {int(np.mean(best_iterations))}\")\n    \n    # --- Retrain on ALL data with avg best iteration ---\n    avg_best = int(np.mean(best_iterations))\n    final_scaler = StandardScaler()\n    X_all_scaled = final_scaler.fit_transform(X)\n    \n    final_model = XGBRegressor(\n        n_estimators=avg_best,      # 用 CV 找到的最佳迭代數\n        max_depth=4,\n        learning_rate=0.03,\n        subsample=0.7,\n        colsample_bytree=0.7,\n        colsample_bylevel=0.7,\n        reg_alpha=1.0,\n        reg_lambda=5.0,\n        min_child_weight=10,\n        gamma=0.3,\n        random_state=42,\n        n_jobs=-1,\n        verbosity=0\n    )\n    final_model.fit(X_all_scaled, y)\n    print(f\"\\n  Final model retrained on ALL data with n_estimators={avg_best}\")\n    \n    results = {\n        \"Parameter\": param_name,\n        \"CV_R2_Train_Mean\": np.mean(cv_r2_train),\n        \"CV_R2_Train_Std\": np.std(cv_r2_train),\n        \"CV_R2_Val_Mean\": np.mean(cv_r2_test),\n        \"CV_R2_Val_Std\": np.std(cv_r2_test),\n        \"CV_RMSE_Val_Mean\": np.mean(cv_rmse_test),\n        \"CV_RMSE_Val_Std\": np.std(cv_rmse_test),\n    }\n    return final_model, final_scaler, pd.DataFrame([results])"
    },
    {
      "cell_type": "markdown",
      "id": "1783a30e-ee66-4c50-a957-b67296e9328c",
      "metadata": {
        "name": "cell34",
        "codeCollapsed": true
      },
      "source": "### Feature Selection & Model Training\n\nBased on feature importance analysis, we drop consistently low-importance features that add noise:\n- `swe` (snow water equivalent — always 0 in South Africa)\n- `bare_ratio`, `med_blue` (very low gain across all targets)\n\nThen we train with 5-fold CV."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d5f62b-585c-45e0-8fa9-73a5fa1248f8",
      "metadata": {
        "language": "python",
        "name": "cell36"
      },
      "outputs": [],
      "source": "X = wq_data.iloc[:, 6:]\n\n# --- 移除低重要性 / 噪音特徵 ---\nlow_importance_features = ['swe', 'bare_ratio', 'med_blue']\ndrop_cols = [c for c in low_importance_features if c in X.columns]\nif drop_cols:\n    print(f\"Dropping low-importance features: {drop_cols}\")\n    X = X.drop(columns=drop_cols)\n\nprint(f\"Final feature count: {X.shape[1]}\")\nprint(f\"Features: {list(X.columns)}\")\n\ny_TA = wq_data['Total Alkalinity']\ny_EC = wq_data['Electrical Conductance']\ny_DRP = wq_data['Dissolved Reactive Phosphorus']\n\nmodel_TA, scaler_TA, results_TA = run_pipeline_cv(X, y_TA, \"Total Alkalinity\")\nmodel_EC, scaler_EC, results_EC = run_pipeline_cv(X, y_EC, \"Electrical Conductance\")\nmodel_DRP, scaler_DRP, results_DRP = run_pipeline_cv(X, y_DRP, \"Dissolved Reactive Phosphorus\")"
    },
    {
      "cell_type": "markdown",
      "id": "644a854c-a8af-411e-b5af-e78d064d80a1",
      "metadata": {
        "name": "cell37",
        "codeCollapsed": true
      },
      "source": "### Model Performance Summary"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f3a047-a05c-4369-8a84-5b287f1ff272",
      "metadata": {
        "language": "python",
        "name": "cell39"
      },
      "outputs": [],
      "source": "results_summary = pd.concat([results_TA, results_EC, results_DRP], ignore_index=True)\nresults_summary"
    },
    {
      "cell_type": "markdown",
      "id": "feat-imp-md",
      "metadata": {
        "name": "cell_fi_md",
        "codeCollapsed": true
      },
      "source": "### Feature Importance (XGBoost)\n\nXGBoost provides built-in feature importance based on how frequently each feature is used in splits (weight), or how much it reduces the loss (gain). This helps identify which satellite bands and climate features matter most for water quality prediction."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feat-imp-code",
      "metadata": {
        "language": "python",
        "name": "cell_fi_code"
      },
      "outputs": [],
      "source": "feature_names = X.columns.tolist()\n\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\nfor ax, model, name in zip(axes, [model_TA, model_EC, model_DRP],\n                            ['Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']):\n    importances = model.feature_importances_\n    sorted_idx = np.argsort(importances)\n    ax.barh(range(len(sorted_idx)), importances[sorted_idx], align='center')\n    ax.set_yticks(range(len(sorted_idx)))\n    ax.set_yticklabels([feature_names[i] for i in sorted_idx])\n    ax.set_title(f'{name}')\n    ax.set_xlabel('Feature Importance (Gain)')\n\nplt.suptitle('XGBoost Feature Importance', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "id": "b5277b08-d7bc-4b7c-91bf-26fa19795f56",
      "metadata": {
        "name": "cell41",
        "codeCollapsed": true
      },
      "source": "## Submission"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1475b7c2-6562-4274-9bd6-98b305010351",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "test_file = pd.read_csv(\"submission_template.csv\")\ndisplay(test_file)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e85fe2-7570-432e-b1f2-bc16dda08ab4",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "landsat_val_features = pd.read_csv(\"landsat_features_validation_200m.csv\")\ndisplay(landsat_val_features)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e53ead-e1b6-4d7d-93bb-12505fbb2ead",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "Terraclimate_val_df = pd.read_csv(\"terraclimate_features_validation_full.csv\")\ndisplay(Terraclimate_val_df)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51991f3-cdc0-4ef3-b173-10b0ff8348a2",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "urbanization_val_df = pd.read_csv(\"urbanization_val.csv\")\n\ncat_cols = [\"Total Alkalinity\", \"Electrical Conductance\", \"Dissolved Reactive Phosphorus\"]\n\nurbanization_val_df = urbanization_val_df.drop(columns=cat_cols)\ndisplay(urbanization_val_df)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4413da47-87cd-48e9-8aaf-cae90b0f46ab",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "cols_to_drop = [\n    'Total Alkalinity',\n    'Electrical Conductance',\n    'Dissolved Reactive Phosphorus'\n]\n\ntest_file = test_file.drop(columns=cols_to_drop, errors='ignore')\n\nval_data = combine_three_datasets(test_file, landsat_val_features, Terraclimate_val_df)\ndisplay(val_data.head(5))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c41d268-0657-490b-94f3-8ffeb67c2265",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell48"
      },
      "outputs": [],
      "source": "# Impute the missing values\nval_data = val_data.fillna(val_data.median(numeric_only=True))\nval_data"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3126bf77-fb54-4609-a09c-8e36b496d108",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell49"
      },
      "outputs": [],
      "source": "# Extracting feature columns from the validation dataset\nsubmission_val_data = val_data.iloc[:, 3:]\n\n# Drop the same low-importance features as training\nlow_importance_features = ['swe', 'bare_ratio', 'med_blue']\ndrop_cols = [c for c in low_importance_features if c in submission_val_data.columns]\nif drop_cols:\n    print(f\"Dropping low-importance features from validation: {drop_cols}\")\n    submission_val_data = submission_val_data.drop(columns=drop_cols)\n\nprint(f\"Validation feature count: {submission_val_data.shape[1]}\")\ndisplay(submission_val_data.head())"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b033c7-3d5f-4179-b8f8-182f8caad0dc",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell50"
      },
      "outputs": [],
      "source": "submission_val_data.shape"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b59132-ff14-421e-b37b-472a0adcd9da",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell51"
      },
      "outputs": [],
      "source": "# --- Predicting for Total Alkalinity ---\nX_sub_scaled_TA = scaler_TA.transform(submission_val_data)\npred_TA_submission = model_TA.predict(X_sub_scaled_TA)\n\n# --- Predicting for Electrical Conductance ---\nX_sub_scaled_EC = scaler_EC.transform(submission_val_data)\npred_EC_submission = model_EC.predict(X_sub_scaled_EC)\n\n# --- Predicting for Dissolved Reactive Phosphorus ---\nX_sub_scaled_DRP = scaler_DRP.transform(submission_val_data)\npred_DRP_submission = model_DRP.predict(X_sub_scaled_DRP)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398e375c-60cc-4fa5-a697-ae12fdab300c",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell52"
      },
      "outputs": [],
      "source": "submission_df = pd.DataFrame({\n    'Latitude': test_file['Latitude'].values,\n    'Longitude': test_file['Longitude'].values,\n    'Sample Date': test_file['Sample Date'].values,\n    'Total Alkalinity': pred_TA_submission,\n    'Electrical Conductance': pred_EC_submission,\n    'Dissolved Reactive Phosphorus': pred_DRP_submission\n})"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e1c4fa-e49f-40cf-ac10-729b82c4b37b",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell53"
      },
      "outputs": [],
      "source": "#Displaying the sample submission dataframe\ndisplay(submission_df.head())"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3a8c41-dba2-43e4-b84c-a50a096980e7",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell54"
      },
      "outputs": [],
      "source": "submission_df.to_csv(\"/tmp/submission_v10.csv\",index = False)\n\nsession.sql(\"\"\"\n    PUT file:///tmp/submission_v10.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")\n\n"
    },
    {
      "cell_type": "markdown",
      "id": "9b7af545-7a55-4b59-9edc-bc43f47bffd5",
      "metadata": {
        "collapsed": false,
        "name": "cell55",
        "codeCollapsed": true
      },
      "source": "### Upload submission file on platform\n\nUpload the `submission.csv` file on the challenge platform to generate your score on the leaderboard."
    },
    {
      "cell_type": "markdown",
      "id": "b7ab09ba-3b69-4a2a-a56e-2dbf084e0c56",
      "metadata": {
        "collapsed": false,
        "name": "cell57",
        "codeCollapsed": true
      },
      "source": "## Conclusion\n\nThis notebook improves on the Random Forest benchmark with the following optimizations:\n\n1. **XGBoost Regression** — gradient boosting that learns from residuals\n2. **5-Fold Cross-Validation** — robust, unbiased performance estimates (no data leakage)\n3. **Stronger regularization** — L1=1.0, L2=5.0, gamma=0.3, min_child_weight=10\n4. **Conservative tree depth** — max_depth=4 to prevent overfitting\n5. **Feature selection** — removed low-importance noise features\n6. **Final model** retrained on ALL data using the avg best iteration from CV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbformat_minor": 5,
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}