{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "lastEditStatus": {
      "notebookId": "i4xqwcgvztmx3skl6dw6",
      "authorId": "8166278287717",
      "authorName": "DATACHALLENGE",
      "authorEmail": "datachallenge@ey.com",
      "sessionId": "c3257d2e-3473-4308-a007-79d96f8cf3b6",
      "lastEditTime": 1765538844113
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b0624afc-f373-48de-a88f-fd70a8cf46b7",
      "metadata": {
        "name": "cell23",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## 2026 EY AI & Data Challenge - Landsat Data Extraction Notebook\n\nThis notebook demonstrates Landsat data extraction and the creation of an output file to be used by the benchmark notebook. The baseline data is [Landsat Collection 2 Level 2](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2) data from the MS Planetary Computer catalog.\n\n**Caution**... This notebook requires significant execution time as there are 9,319 data points (unique locations and times) used for data extraction from the Landsat archive. The code takes about 7 hours to run to completion on a typical laptop computer with a typical internet connection. Lower execution times are likely possible with optimization of the data extraction process and the use of cloud computing services.\n"
    },
    {
      "cell_type": "markdown",
      "id": "cf044936-9aad-4300-a873-ddf8d2b43835",
      "metadata": {
        "name": "cell2",
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "### Load In Dependencies\nThe following code installs the required Python libraries (found in the requirements.txt file) in the Snowflake environment to allow successful execution of the remaining notebook code. After running this code for the first time, it is required to â€œrestartâ€ the kernal so the Python libraries are available in the environment. This is done by selecting the â€œConnectedâ€ menu above the notebook (next to â€œRun allâ€) and selecting the â€œrestart kernalâ€ link. Subsequent runs of the notebook do not require this â€œrestartâ€ process. "
    },
    {
      "cell_type": "code",
      "id": "d0759bc3-bff9-4cfe-83d0-c1f808697c9d",
      "metadata": {
        "language": "python",
        "name": "cell26"
      },
      "outputs": [],
      "source": "!pip install uv\n!uv pip install  -r requirements.txt ",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a09f2097-d79a-4a87-9977-79a85b8e651b",
      "metadata": {
        "name": "cell3",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "import snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\n\n# Planetary Computer tools for STAC API access and authentication\nimport pystac_client\nimport planetary_computer as pc\nfrom odc.stac import stac_load\nfrom pystac.extensions.eo import EOExtension as eo\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os\nimport time\nimport re\n\ntqdm.pandas()  "
    },
    {
      "cell_type": "markdown",
      "id": "d7f6736c-ac2e-44d2-bded-d3266897074e",
      "metadata": {
        "name": "cell1",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Extracting Landsat Data Using API Calls\n\nThe API-based method allows us to efficiently access **Landsat** data for specific coordinates and time periods, ensuring scalability and reproducibility of the process.\n\nThrough the API, we can query individual bands or compute indices like **NDMI** on the fly. This approach reduces storage requirements and simplifies data preprocessing, making it ideal for large-scale environmental and water quality analysis.\n\nThe **compute_Landsat_values** function extracts Landsat surface reflectance values for specific sampling locations using a 100 m focal buffer around each point. For each location:\n\n- A bounding box (bbox) is created around the latitude and longitude coordinates.\n- The Microsoft Planetary Computer API is queried for Landsat-8 Level-2 surface reflectance imagery within the date range.\n- The nearest low-cloud (<10% cloud cover) scene is selected, and the specified bands (**green**, **nir08**, **swir16**, **swir22**) are loaded.\n- Median values of the pixels within the bounding box are computed to reduce the effect of noise or outliers.\n\n**Why the buffer value is 0.00089831**\n\nWe want a ~100 m buffer around each point.  \nAt the equator, 1 degree â‰ˆ 110 km.\n\nTherefore, the degree equivalent of 100 m is:\n\n*buffer_deg â‰ˆ 100 m / 110,000 m per degree â‰ˆ 0.00089831*\n\nThis value ensures that the buffer approximately matches the pixel resolution of Landsat imagery, capturing a ~100 m area around each sampling location.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dcd447c8-2951-4391-a5c8-916ce9666306",
      "metadata": {
        "name": "cell5",
        "language": "python"
      },
      "outputs": [],
      "source": "# åŸç‰ˆçš„code\n# # Setup\n# tqdm.pandas()\n\n# def compute_Landsat_values(row):\n#     lat = row['Latitude']\n#     lon = row['Longitude']\n#     date = pd.to_datetime(row['Sample Date'], dayfirst=True, errors='coerce')\n\n#     # Buffer size for ~100m \n#     bbox_size = 0.00089831  \n#     bbox = [\n#         lon - bbox_size / 2,\n#         lat - bbox_size / 2,\n#         lon + bbox_size / 2,\n#         lat + bbox_size / 2\n#     ]\n\n#     catalog = pystac_client.Client.open(\n#         \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n#         modifier=pc.sign_inplace,\n#     )\n\n#     # Wider search range, we'll filter to nearest date later\n#     search = catalog.search(\n#         collections=[\"landsat-c2-l2\"],\n#         bbox=bbox,\n#         datetime=\"2011-01-01/2015-12-31\",\n#         query={\"eo:cloud_cover\": {\"lt\": 10}},\n#     )\n    \n#     items = search.item_collection()\n\n#     if not items:\n#         return pd.Series({\n#             \"nir\": np.nan, \"green\": np.nan, \"swir16\": np.nan, \"swir22\": np.nan\n#         })\n\n#     try:\n#         # Convert sample date to UTC\n#         sample_date_utc = date.tz_localize(\"UTC\") if date.tzinfo is None else date.tz_convert(\"UTC\")\n\n#         # Pick the item closest to the sample date\n#         items = sorted(\n#             items,\n#             key=lambda x: abs(pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc)\n#         )\n#         selected_item = pc.sign(items[0])\n\n#         # Load required bands\n#         bands_of_interest = [\"green\", \"nir08\", \"swir16\", \"swir22\"]\n#         data = stac_load([selected_item], bands=bands_of_interest, bbox=bbox).isel(time=0)\n\n#         green = data[\"green\"].astype(\"float\")\n#         nir = data[\"nir08\"].astype(\"float\")\n#         swir16 = data[\"swir16\"].astype(\"float\")\n#         swir22 = data[\"swir22\"].astype(\"float\")\n        \n#         # Compute medians\n#         median_green = float(green.median(skipna=True).values)\n#         median_nir = float(nir.median(skipna=True).values)\n#         median_swir16 = float(swir16.median(skipna=True).values)\n#         median_swir22 = float(swir22.median(skipna=True).values)\n\n#         # Replace 0 with NaN\n#         median_green = median_green if median_green != 0 else np.nan\n#         median_nir = median_nir if median_nir != 0 else np.nan\n#         median_swir16 = median_swir16 if median_swir16 != 0 else np.nan\n#         median_swir22 = median_swir22 if median_swir22 != 0 else np.nan\n        \n#         return pd.Series({\n#             \"nir\": median_nir,\n#             \"green\": median_green,\n#             \"swir16\": median_swir16,\n#             \"swir22\": median_swir22,\n#         })\n    \n#     except Exception as e:\n#         return pd.Series({\n#             \"nir\": np.nan, \"green\": np.nan, \"swir16\": np.nan, \"swir22\": np.nan\n#         })"
    },
    {
      "cell_type": "markdown",
      "id": "a9a4ea22-9d2a-4866-bc59-bf3962ecfe1a",
      "metadata": {
        "name": "cell6",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Extracting features for the training dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3c046f70-aa61-4bbe-8b1a-ac8624f87ed9",
      "metadata": {
        "name": "cell7",
        "language": "python"
      },
      "outputs": [],
      "source": "Water_Quality_df=pd.read_csv('water_quality_training_dataset.csv')\ndisplay(Water_Quality_df.head())"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f33406f9-9c07-400b-95a5-cdfdc81c5eba",
      "metadata": {
        "name": "cell8",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "Water_Quality_df.shape"
    },
    {
      "cell_type": "markdown",
      "id": "ad8b49cf-b76b-4633-be3e-caf4354ff0ef",
      "metadata": {
        "name": "cell4",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Note\n\nThe Landsat data extraction process for all 9,319 locations typically requires more than 7 hours when executed in a single run. During long executions, you may occasionally encounter API limits, timeout errors, or request failures. To avoid these interruptions, we recommend running the extraction in smaller batches.\n\nIn this notebook, we provide a sample code snippet demonstrating how to extract data for the first 200 locations. Participants are encouraged to follow the same batching approach to extract data for all 9,319 locations safely and efficiently.\n\nWe have already executed the full extraction for all 9,319 locations and saved the output to **landsat_features_training.csv**, which will be used in the benchmark notebook.  \nSimilarly, participants can extract Landsat features in batches, combine the batch outputs, and save the final merged dataset as **landsat_features_training.csv** to ensure the benchmark notebook runs smoothly.\n"
    },
    {
      "id": "4727ee84-d658-4696-af15-f1561f0a2a20",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### APIæ’ˆå–Landsatçš„rawæª”ï¼ˆå®˜æ–¹è³‡æ–™ç‰ˆï¼‰"
    },
    {
      "id": "7a00e263-4b30-45ce-90f8-6ed64dcb26ce",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def compute_Landsat_raw(row):\n    lat = row['Latitude']\n    lon = row['Longitude']\n    date = pd.to_datetime(row['Sample Date'], dayfirst=True, errors='coerce')\n\n    bbox_size = 0.00089831*2\n    bbox = [\n        lon - bbox_size / 2,\n        lat - bbox_size / 2,\n        lon + bbox_size / 2,\n        lat + bbox_size / 2\n    ]\n\n    catalog = pystac_client.Client.open(\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n        modifier=pc.sign_inplace,\n    )\n\n    search = catalog.search(\n        collections=[\"landsat-c2-l2\"],\n        bbox=bbox,\n        datetime=\"2011-01-01/2015-12-31\",\n        query={\"eo:cloud_cover\": {\"lt\": 10}},\n    )\n\n    items = search.item_collection()\n\n    if not items:\n        return pd.Series({\n            \"blue\": None,\n            \"green\": None,\n            \"red\": None,\n            \"nir\": None,\n            \"swir16\": None,\n            \"swir22\": None,\n        })\n\n    try:\n        sample_date_utc = date.tz_localize(\"UTC\") if date.tzinfo is None else date.tz_convert(\"UTC\")\n\n        items = sorted(\n            items,\n            key=lambda x: abs(\n                pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc\n            )\n        )\n        selected_item = pc.sign(items[0])\n\n        bands = [\"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"swir22\"]\n        data = stac_load([selected_item], bands=bands, bbox=bbox).isel(time=0)\n\n        return pd.Series({\n            \"blue\": data[\"blue\"].values.flatten().tolist(),\n            \"green\": data[\"green\"].values.flatten().tolist(),\n            \"red\": data[\"red\"].values.flatten().tolist(),\n            \"nir\": data[\"nir08\"].values.flatten().tolist(),\n            \"swir16\": data[\"swir16\"].values.flatten().tolist(),\n            \"swir22\": data[\"swir22\"].values.flatten().tolist(),\n        })\n\n    except Exception:\n        return pd.Series({\n            \"blue\": None,\n            \"green\": None,\n            \"red\": None,\n            \"nir\": None,\n            \"swir16\": None,\n            \"swir22\": None,\n        })\n\n\ndef run_landsat_raw_in_batches(\n    df,\n    out_dir,\n    batch_size,          # å…ˆç”¨ 100~300 æ¯”è¼ƒå®‰å…¨\n    start_batch,\n    pause_every,           # æ¯è·‘å¹¾å€‹ batch å°±ä¼‘æ¯\n    pause_seconds       # ä¼‘æ¯ç§’æ•¸ï¼ˆ3 åˆ†é˜ï¼‰\n):\n    os.makedirs(out_dir, exist_ok=True)\n\n    # åªå–å¿…è¦æ¬„ä½ä¸¦å»é‡\n    points = (\n        df[['Latitude', 'Longitude', 'Sample Date']]\n        .drop_duplicates()\n        .reset_index(drop=True)\n    )\n\n    n = len(points)\n    n_batches = (n + batch_size - 1) // batch_size\n\n    print(f\"Total unique points: {n}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Total batches: {n_batches}\")\n    print(f\"Pause: every {pause_every} batches, sleep {pause_seconds} seconds\")\n\n    completed_this_run = 0\n\n    for b in range(start_batch, n_batches):\n        out_path = f\"{out_dir}/landsat_raw_batch_{b:05d}.parquet\"\n\n        s = b * batch_size\n        e = min((b + 1) * batch_size, n)\n        batch_points = points.iloc[s:e].reset_index(drop=True)\n\n        print(f\"ğŸš€ Running batch {b}/{n_batches-1}: rows {s}..{e-1} ({len(batch_points)})\")\n\n        # å‘¼å« API\n        batch_raw = batch_points.progress_apply(\n            compute_Landsat_raw,\n            axis=1\n        )\n\n        batch_df = pd.concat(\n            [\n                batch_points.reset_index(drop=True),\n                batch_raw.reset_index(drop=True)\n            ],\n            axis=1\n        )\n\n        # ç›´æ¥å¯«æª”ï¼ˆè‹¥å­˜åœ¨æœƒè¦†è“‹ï¼‰\n        batch_df.to_parquet(out_path, index=False)\n        print(f\"âœ… Saved (overwritten if existed): {out_path}\")\n\n        completed_this_run += 1\n\n        # ä¼‘æ¯æ©Ÿåˆ¶\n        if pause_every and completed_this_run % pause_every == 0:\n            print(\n                f\"ğŸ§˜ Completed {completed_this_run} batches. \"\n                f\"Sleeping {pause_seconds/60:.0f} minutes...\"\n            )\n            time.sleep(pause_seconds)\n\n    print(\"ğŸ‰ All batches done.\")\n\n# -----------------------------\n# Merge all batch parquets -> one parquet\n# -----------------------------\ndef merge_landsat_batches(\n    out_dir,\n    merged_path\n):\n    if not os.path.isdir(out_dir):\n        raise FileNotFoundError(f\"Folder not found: {out_dir}\")\n\n    # åªæŠ“ç¬¦åˆä½  batch å‘½åè¦å‰‡çš„æª”æ¡ˆï¼šlandsat_raw_batch_00000.parquet\n    pattern = re.compile(r\"^landsat_raw_batch_(\\d{5})\\.parquet$\")\n\n    batch_files = []\n    for fn in os.listdir(out_dir):\n        m = pattern.match(fn)\n        if m:\n            batch_id = int(m.group(1))\n            batch_files.append((batch_id, os.path.join(out_dir, fn)))\n\n    if not batch_files:\n        raise FileNotFoundError(f\"No batch parquet files found in {out_dir}\")\n\n    # ç”¨ batch ç·¨è™Ÿæ’åºï¼ˆæœ€ç©©ï¼‰\n    batch_files.sort(key=lambda x: x[0])\n    files = [p for _, p in batch_files]\n\n    # è®€ç¬¬ä¸€å€‹æª”æ¡ˆç•¶æ¬„ä½åŸºæº–ï¼Œé¿å…æ¬„ä½ä¸ä¸€è‡´æ™‚ concat é»˜é»˜è£œ NaN\n    df0 = pd.read_parquet(files[0])\n    base_cols = list(df0.columns)\n\n    dfs = [df0]\n    mismatched = []\n\n    for f in files[1:]:\n        dfi = pd.read_parquet(f)\n\n        if list(dfi.columns) != base_cols:\n            mismatched.append(f)\n            # å°é½Šæ¬„ä½ï¼ˆå¤šçš„ä¸Ÿæ‰ï¼Œå°‘çš„è£œ NaNï¼‰ï¼ŒåŒæ™‚æé†’ä½ æœ‰ç•°å¸¸ batch\n            dfi = dfi.reindex(columns=base_cols)\n\n        dfs.append(dfi)\n\n    df_all = pd.concat(dfs, ignore_index=True)\n    df_all.to_parquet(merged_path, index=False)\n\n    print(f\"âœ… Merged {len(files)} files -> {merged_path}\")\n    print(\"Shape:\", df_all.shape)\n\n    if mismatched:\n        print(f\"âš ï¸ Column mismatch detected in {len(mismatched)} files (aligned to base columns).\")\n        # æƒ³çœ‹æ¸…å–®å¯ä»¥å–æ¶ˆä¸‹ä¸€è¡Œè¨»è§£\n        # print(\"\\n\".join(mismatched))\n\n    return df_all\n\n\nrun_landsat_raw_in_batches(\n    df=Water_Quality_df,\n    out_dir=\"landsat_raw_batches\",\n    start_batch=0,\n    batch_size=100,\n    pause_every=3,\n    pause_seconds=180\n)\n\nlandsat_raw_all = merge_landsat_batches(\n    out_dir=\"landsat_raw_batches\",\n    merged_path=\"/tmp/landsat_raw_all.parquet\"\n)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f54f3e4c-1ccc-4499-8633-72c907a61732",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### APIæ’ˆå–Landsatçš„rawæª”ï¼ˆé­”æ”¹åŠ sampleç‰ˆï¼‰"
    },
    {
      "id": "4acce91c-eb1f-4848-a5e7-330031ba70c3",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def compute_Landsat_raw(row):\n    import numpy as np\n    import pandas as pd\n\n    lat = row['Latitude']\n    lon = row['Longitude']\n    date = pd.to_datetime(row['Sample Date'], dayfirst=True, errors='coerce')\n\n    bbox_size = 0.00089831\n    bbox = [\n        lon - bbox_size / 2,\n        lat - bbox_size / 2,\n        lon + bbox_size / 2,\n        lat + bbox_size / 2\n    ]\n\n    catalog = pystac_client.Client.open(\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n        modifier=pc.sign_inplace,\n    )\n\n    # ä¸ç”¨ eo:cloud_coverï¼›æ”¹ç”¨ pixel-level QA é®ç½©\n    search = catalog.search(\n        collections=[\"landsat-c2-l2\"],\n        bbox=bbox,\n        datetime=\"2011-01-01/2015-12-31\",\n    )\n\n    items = search.item_collection()\n\n    if not items:\n        return pd.Series({\n            \"blue\": None, \"green\": None, \"red\": None, \"nir\": None,\n            \"swir16\": None, \"swir22\": None, \"st_b10\": None,\n        })\n\n    try:\n        # 1) é¸æœ€è¿‘æ—¥æœŸçš„ scene\n        sample_date_utc = date.tz_localize(\"UTC\") if date.tzinfo is None else date.tz_convert(\"UTC\")\n        items = sorted(\n            items,\n            key=lambda x: abs(pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc)\n        )\n        selected_item = pc.sign(items[0])\n\n        # 2) è®€å– bandï¼ˆSR + ST + QAï¼‰\n        bands = [\n            \"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"swir22\",\n            \"ST_B10\",\n            \"QA_PIXEL\", \"QA_RADSAT\", \"SR_QA_AEROSOL\",\n        ]\n\n        # âœ… é—œéµï¼šQA ç”¨ nearestï¼Œé€£çºŒå€¼ band å¯ç”¨ bilinearï¼ˆæˆ–ä½ ä¹Ÿå¯å…¨éƒ¨ nearest æ›´ä¿å®ˆï¼‰\n        data = stac_load(\n            [selected_item],\n            bands=bands,\n            bbox=bbox,\n            resampling={\n                \"QA_PIXEL\": \"nearest\",\n                \"QA_RADSAT\": \"nearest\",\n                \"SR_QA_AEROSOL\": \"nearest\",\n                \"*\": \"bilinear\",\n            }\n        ).isel(time=0)\n\n        # 3) å–å‡º QAï¼ˆæ•´æ•¸ï¼‰\n        qa_pixel = data[\"QA_PIXEL\"].values.astype(np.uint16)\n        qa_radsat = data[\"QA_RADSAT\"].values.astype(np.uint16)\n        qa_aerosol = data[\"SR_QA_AEROSOL\"].values.astype(np.uint8)\n\n        def bit_is_set(arr, bit):\n            return (arr & (1 << bit)) != 0\n\n        # QA_PIXEL bits: fill/dilated cloud/cirrus/cloud/shadow/snow -> å…¨æ’é™¤\n        mask_clear = (\n            (~bit_is_set(qa_pixel, 0)) &\n            (~bit_is_set(qa_pixel, 1)) &\n            (~bit_is_set(qa_pixel, 2)) &\n            (~bit_is_set(qa_pixel, 3)) &\n            (~bit_is_set(qa_pixel, 4)) &\n            (~bit_is_set(qa_pixel, 5))\n        )\n\n        # QA_RADSATï¼šä»»ä½•é£½å’Œéƒ½æ’é™¤\n        mask_nosat = (qa_radsat == 0)\n\n        # SR_QA_AEROSOLï¼šbits 6-7 = aerosol levelï¼Œæ’é™¤ high (=3)ï¼›bit0=fill ä¹Ÿæ’é™¤\n        aerosol_level = (qa_aerosol >> 6) & 0b11\n        mask_aerosol_ok = (aerosol_level != 3) & ((qa_aerosol & 1) == 0)\n\n        mask_valid = mask_clear & mask_nosat & mask_aerosol_ok\n\n        # 4) SR/ST è½‰ç‰©ç†é‡ï¼ˆscale/offsetï¼‰\n        # SR: reflectance = DN*0.0000275 - 0.2\n        # ST_B10: temp(K) = DN*0.00341802 + 149.0\n        def sr_to_reflectance(da):\n            dn = da.values.astype(\"float32\")\n            return dn * 0.0000275 - 0.2\n\n        def st_to_kelvin(da):\n            dn = da.values.astype(\"float32\")\n            return dn * 0.00341802 + 149.0\n\n        blue = sr_to_reflectance(data[\"blue\"])\n        green = sr_to_reflectance(data[\"green\"])\n        red = sr_to_reflectance(data[\"red\"])\n        nir = sr_to_reflectance(data[\"nir08\"])\n        swir16 = sr_to_reflectance(data[\"swir16\"])\n        swir22 = sr_to_reflectance(data[\"swir22\"])\n        st_b10 = st_to_kelvin(data[\"ST_B10\"])  # å…ˆç”¨ Kelvinï¼›ä½ æƒ³è¦ Â°C æˆ‘ä¹Ÿå¯å¹«ä½ å¤šå­˜ä¸€æ¬„\n\n        # 5) å¥— maskï¼Œä¸¦ã€Œæ¯”ç…§èˆŠç‰ˆã€è¼¸å‡º list\n        def masked_list(arr):\n            arr = arr.astype(\"float32\")\n            arr[~mask_valid] = np.nan\n            return arr.flatten().tolist()\n\n        return pd.Series({\n            \"blue\": masked_list(blue),\n            \"green\": masked_list(green),\n            \"red\": masked_list(red),\n            \"nir\": masked_list(nir),\n            \"swir16\": masked_list(swir16),\n            \"swir22\": masked_list(swir22),\n            \"st_b10\": masked_list(st_b10),\n        })\n\n    except Exception:\n        return pd.Series({\n            \"blue\": None, \"green\": None, \"red\": None, \"nir\": None,\n            \"swir16\": None, \"swir22\": None, \"st_b10\": None,\n        })\n\n\n\n# -----------------------------\n# Merge all batch parquets -> one parquet\n# -----------------------------\ndef merge_landsat_batches(\n    out_dir,\n    merged_path\n):\n    if not os.path.isdir(out_dir):\n        raise FileNotFoundError(f\"Folder not found: {out_dir}\")\n\n    # åªæŠ“ç¬¦åˆä½  batch å‘½åè¦å‰‡çš„æª”æ¡ˆï¼šlandsat_raw_batch_00000.parquet\n    pattern = re.compile(r\"^landsat_raw_batch_(\\d{5})\\.parquet$\")\n\n    batch_files = []\n    for fn in os.listdir(out_dir):\n        m = pattern.match(fn)\n        if m:\n            batch_id = int(m.group(1))\n            batch_files.append((batch_id, os.path.join(out_dir, fn)))\n\n    if not batch_files:\n        raise FileNotFoundError(f\"No batch parquet files found in {out_dir}\")\n\n    # ç”¨ batch ç·¨è™Ÿæ’åºï¼ˆæœ€ç©©ï¼‰\n    batch_files.sort(key=lambda x: x[0])\n    files = [p for _, p in batch_files]\n\n    # è®€ç¬¬ä¸€å€‹æª”æ¡ˆç•¶æ¬„ä½åŸºæº–ï¼Œé¿å…æ¬„ä½ä¸ä¸€è‡´æ™‚ concat é»˜é»˜è£œ NaN\n    df0 = pd.read_parquet(files[0])\n    base_cols = list(df0.columns)\n\n    dfs = [df0]\n    mismatched = []\n\n    for f in files[1:]:\n        dfi = pd.read_parquet(f)\n\n        if list(dfi.columns) != base_cols:\n            mismatched.append(f)\n            # å°é½Šæ¬„ä½ï¼ˆå¤šçš„ä¸Ÿæ‰ï¼Œå°‘çš„è£œ NaNï¼‰ï¼ŒåŒæ™‚æé†’ä½ æœ‰ç•°å¸¸ batch\n            dfi = dfi.reindex(columns=base_cols)\n\n        dfs.append(dfi)\n\n    df_all = pd.concat(dfs, ignore_index=True)\n    df_all.to_parquet(merged_path, index=False)\n\n    print(f\"âœ… Merged {len(files)} files -> {merged_path}\")\n    print(\"Shape:\", df_all.shape)\n\n    if mismatched:\n        print(f\"âš ï¸ Column mismatch detected in {len(mismatched)} files (aligned to base columns).\")\n        # æƒ³çœ‹æ¸…å–®å¯ä»¥å–æ¶ˆä¸‹ä¸€è¡Œè¨»è§£\n        # print(\"\\n\".join(mismatched))\n\n    return df_all\n\n\nrun_landsat_raw_in_batches(\n    df=Water_Quality_df,\n    out_dir=\"landsat_raw_batches\",\n    start_batch=13,\n    batch_size=100,\n    pause_every=3,\n    pause_seconds=5\n)\n\nlandsat_raw_all = merge_landsat_batches(\n    out_dir=\"landsat_raw_batches\",\n    merged_path=\"/tmp/landsat_raw_all.parquet\"\n)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d3019c7e-4564-4880-88d8-cc5d147e1d12",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### å„²å­˜landsatçš„rawæª”ï¼Œè½‰å­˜github"
    },
    {
      "id": "4ac58231-6b57-4c28-8d23-7bb156a28b86",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "landsat_raw_all = merge_landsat_batches(\n    out_dir=\"landsat_raw_batches\",\n    merged_path=\"/tmp/landsat_raw_all_v2.parquet\"\n)\n\nsession.sql(\"\"\"\n    PUT file:///tmp/landsat_raw_all_v2.parquet\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9b4ada3d-b7bb-4292-9dc8-2896ddc3f4e3",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### æ¸…ç†æ‰æ‰€æœ‰ç”¨APIæ’ˆå–landsatçš„rawæª”çš„æš«å­˜"
    },
    {
      "id": "67c187d3-1f84-4372-b9e2-704029bda166",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import os\n\ndef clear_landsat_batch_dir(out_dir=\"landsat_raw_batches\"):\n    if not os.path.exists(out_dir):\n        print(f\"ğŸ“‚ Directory does not exist: {out_dir}\")\n        return\n\n    files = os.listdir(out_dir)\n\n    if not files:\n        print(f\"ğŸ“‚ Directory already empty: {out_dir}\")\n        return\n\n    for f in files:\n        path = os.path.join(out_dir, f)\n        if os.path.isfile(path):\n            os.remove(path)\n\n    print(f\"ğŸ§¹ Cleared all files in: {out_dir}\")\n    \nclear_landsat_batch_dir(\"landsat_raw_batches\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ea357dbb-8944-4f80-a252-0cb40e4ed37c",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### ç”¨rawæª”åšå„ç¨®è¨ˆç®—ï¼Œä»¥medianç‚ºä¾‹"
    },
    {
      "id": "60461b6b-cfd6-407b-a29c-179e2679d604",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "path = \"/tmp/landsat_raw.parquet\"\nlandsat_raw = pd.read_parquet(path)\n\ndef compute_median_from_pixels(arr):\n    # å°é½Šä½ åŸæœ¬é‚è¼¯ï¼šmedian(skipna=True) + median==0 -> NaN\n    if arr is None:\n        return np.nan\n\n    a = np.array(arr, dtype=\"float\")\n    a = a[~np.isnan(a)]  # ç­‰åŒ skipna=True\n\n    if len(a) == 0:\n        return np.nan\n\n    median_val = float(np.median(a))\n    return median_val if median_val != 0 else np.nan\n\n\n# 1) å»ºç«‹åªå« key æ¬„ä½çš„è¼¸å‡ºè¡¨\nlandsat_train_features = landsat_raw[['Latitude', 'Longitude', 'Sample Date']].copy()\n\n# 2) å¥—ç”¨ median è¨ˆç®—ï¼ˆå°é½Šä½ è²¼çš„å››å€‹ bandï¼‰\nlandsat_train_features['nir']    = landsat_raw['nir'].apply(compute_median_from_pixels)\nlandsat_train_features['green']  = landsat_raw['green'].apply(compute_median_from_pixels)\nlandsat_train_features['swir16'] = landsat_raw['swir16'].apply(compute_median_from_pixels)\nlandsat_train_features['swir22'] = landsat_raw['swir22'].apply(compute_median_from_pixels)\n\n# 3) å­˜æˆ parquet\nlandsat_train_features.to_csv(\"/tmp/landsat_features_training_jin.csv\", index=False)\n\n\nsession.sql(\"\"\"\n    PUT file:///tmp/landsat_features_training_jin.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")\n\nlandsat_train_features",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d6ccba0e-8418-4d90-bcb8-1f16bf4be640",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "##åŸæœ¬å®˜æ–¹çš„code\n# # Extract band values from Landsat for training dataset\n# train_features_path = \"landsat_features_training.csv\"\n\n# print(\"ğŸš€ Running Landsat feature extraction for training data...\")\n# landsat_train_features = Water_Quality_df.progress_apply(compute_Landsat_values, axis=1)\n# landsat_train_features.to_csv(train_features_path, index=False)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "400e9ddb-81f6-45eb-ad41-e1950f6f4eac",
      "metadata": {
        "name": "cell12",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "**NDMI and MNDWI Indices**\n\nIn this notebook, we compute two commonly used water-related indices from the extracted Landsat bands:\n\n- **NDMI (Normalized Difference Moisture Index):**  \n  Measures vegetation water content and surface moisture.  \n  Computed as *(NIR - SWIR16) / (NIR + SWIR16)*.\n\n- **MNDWI (Modified Normalized Difference Water Index):**  \n  Highlights open water features by enhancing water reflectance and suppressing built-up areas.  \n  Computed as *(Green - SWIR16) / (Green + SWIR16)*.\n\nAn **epsilon value** (*eps = 1e-10*) is added to the denominators to avoid division by zero.  \nThese indices are widely used in hydrological and water quality analyses for detecting water presence and vegetation moisture levels.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "31ca3eaa-45f9-44af-b444-2dff934d02f4",
      "metadata": {
        "name": "cell13",
        "language": "python"
      },
      "outputs": [],
      "source": "# Create indices: NDMI and MNDWI\neps = 1e-10\nlandsat_train_features['NDMI'] = (landsat_train_features['nir'] - landsat_train_features['swir16']) / (landsat_train_features['nir'] + landsat_train_features['swir16'] + eps)\nlandsat_train_features['MNDWI'] = (landsat_train_features['green'] - landsat_train_features['swir16']) / (landsat_train_features['green'] + landsat_train_features['swir16'] + eps)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0d9d6711-3553-437a-8830-db69ce8afc80",
      "metadata": {
        "name": "cell14",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "landsat_train_features['Latitude'] = Water_Quality_df['Latitude']\nlandsat_train_features['Longitude'] = Water_Quality_df['Longitude']\nlandsat_train_features['Sample Date'] = Water_Quality_df['Sample Date']\nlandsat_train_features = landsat_train_features[['Latitude', 'Longitude', 'Sample Date', 'nir', 'green', 'swir16', 'swir22', 'NDMI', 'MNDWI']]"
    },
    {
      "cell_type": "code",
      "id": "97f72aca-7bfe-4016-8757-b0541ed8060b",
      "metadata": {
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": "# Preview File\nlandsat_train_features.head()",
      "execution_count": null
    },
    {
      "id": "5dc6075a-180c-4915-976f-a17846b32473",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "landsat_train_features.to_csv(\"/tmp/landsat_features_training_20.csv\",index = False)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9e89a601-832c-451a-baf2-ed265817c1f8",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "session.sql(\"\"\"\n    PUT file:///tmp/landsat_features_training_20.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "56f44c9f-2336-4fe7-a587-78cf6efe694b",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "**Note:** If you're using your own workspace, remember to replace \"EY-AI-and-Data-Challenge\" with your workspace name in the file path."
    },
    {
      "cell_type": "markdown",
      "id": "1ac5f5ea-bd8a-41bc-bb62-47cc952952bf",
      "metadata": {
        "name": "cell17",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Extracting features for the validation dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb2adc08-4661-4fdc-a72d-78d34d37db73",
      "metadata": {
        "name": "cell18",
        "language": "python"
      },
      "outputs": [],
      "source": "Validation_df=pd.read_csv('submission_template.csv')\ndisplay(Validation_df.head())"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "53ebf740-1917-4af1-b214-d60372f0ad0b",
      "metadata": {
        "name": "cell19",
        "language": "python"
      },
      "outputs": [],
      "source": "Validation_df.shape"
    },
    {
      "id": "c84a91c8-0b8f-4e96-8620-8d4aa18e0599",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "def run_landsat_raw_in_batches(\n    df,\n    out_dir,\n    batch_size,          # å…ˆç”¨ 100~300 æ¯”è¼ƒå®‰å…¨\n    start_batch,\n    pause_every,           # æ¯è·‘å¹¾å€‹ batch å°±ä¼‘æ¯\n    pause_seconds       # ä¼‘æ¯ç§’æ•¸ï¼ˆ3 åˆ†é˜ï¼‰\n):\n    os.makedirs(out_dir, exist_ok=True)\n\n    # åªå–å¿…è¦æ¬„ä½ä¸¦å»é‡\n    points = (\n        df[['Latitude', 'Longitude', 'Sample Date']]\n        .drop_duplicates()\n        .reset_index(drop=True)\n    )\n\n    n = len(points)\n    n_batches = (n + batch_size - 1) // batch_size\n\n    print(f\"Total unique points: {n}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Total batches: {n_batches}\")\n    print(f\"Pause: every {pause_every} batches, sleep {pause_seconds} seconds\")\n\n    completed_this_run = 0\n\n    for b in range(start_batch, n_batches):\n        out_path = f\"{out_dir}/landsat_raw_batch_{b:05d}.parquet\"\n\n        s = b * batch_size\n        e = min((b + 1) * batch_size, n)\n        batch_points = points.iloc[s:e].reset_index(drop=True)\n\n        print(f\"ğŸš€ Running batch {b}/{n_batches-1}: rows {s}..{e-1} ({len(batch_points)})\")\n\n        # å‘¼å« API\n        batch_raw = batch_points.progress_apply(\n            compute_Landsat_raw,\n            axis=1\n        )\n\n        batch_df = pd.concat(\n            [\n                batch_points.reset_index(drop=True),\n                batch_raw.reset_index(drop=True)\n            ],\n            axis=1\n        )\n\n        # ç›´æ¥å¯«æª”ï¼ˆè‹¥å­˜åœ¨æœƒè¦†è“‹ï¼‰\n        batch_df.to_parquet(out_path, index=False)\n        print(f\"âœ… Saved (overwritten if existed): {out_path}\")\n\n        completed_this_run += 1\n\n        # ä¼‘æ¯æ©Ÿåˆ¶\n        if pause_every and completed_this_run % pause_every == 0:\n            print(\n                f\"ğŸ§˜ Completed {completed_this_run} batches. \"\n                f\"Sleeping {pause_seconds/60:.0f} minutes...\"\n            )\n            time.sleep(pause_seconds)\n\n    print(\"ğŸ‰ All batches done.\")\n\n\nrun_landsat_raw_in_batches(\n    df=Validation_df,\n    out_dir=\"landsat_validation_raw_batches\",\n    start_batch=0,\n    batch_size=100,\n    pause_every=3,\n    pause_seconds=180\n)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "927f957b-8cad-411f-bd88-e65e686a19d0",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import re\n\n# -----------------------------\n# Merge all batch parquets -> one parquet\n# -----------------------------\ndef merge_landsat_batches(\n    out_dir,\n    merged_path\n):\n    if not os.path.isdir(out_dir):\n        raise FileNotFoundError(f\"Folder not found: {out_dir}\")\n\n    # åªæŠ“ç¬¦åˆä½  batch å‘½åè¦å‰‡çš„æª”æ¡ˆï¼šlandsat_raw_batch_00000.parquet\n    pattern = re.compile(r\"^landsat_raw_batch_(\\d{5})\\.parquet$\")\n\n    batch_files = []\n    for fn in os.listdir(out_dir):\n        m = pattern.match(fn)\n        if m:\n            batch_id = int(m.group(1))\n            batch_files.append((batch_id, os.path.join(out_dir, fn)))\n\n    if not batch_files:\n        raise FileNotFoundError(f\"No batch parquet files found in {out_dir}\")\n\n    # ç”¨ batch ç·¨è™Ÿæ’åºï¼ˆæœ€ç©©ï¼‰\n    batch_files.sort(key=lambda x: x[0])\n    files = [p for _, p in batch_files]\n\n    # è®€ç¬¬ä¸€å€‹æª”æ¡ˆç•¶æ¬„ä½åŸºæº–ï¼Œé¿å…æ¬„ä½ä¸ä¸€è‡´æ™‚ concat é»˜é»˜è£œ NaN\n    df0 = pd.read_parquet(files[0])\n    base_cols = list(df0.columns)\n\n    dfs = [df0]\n    mismatched = []\n\n    for f in files[1:]:\n        dfi = pd.read_parquet(f)\n\n        if list(dfi.columns) != base_cols:\n            mismatched.append(f)\n            # å°é½Šæ¬„ä½ï¼ˆå¤šçš„ä¸Ÿæ‰ï¼Œå°‘çš„è£œ NaNï¼‰ï¼ŒåŒæ™‚æé†’ä½ æœ‰ç•°å¸¸ batch\n            dfi = dfi.reindex(columns=base_cols)\n\n        dfs.append(dfi)\n\n    df_all = pd.concat(dfs, ignore_index=True)\n    df_all.to_parquet(merged_path, index=False)\n\n    print(f\"âœ… Merged {len(files)} files -> {merged_path}\")\n    print(\"Shape:\", df_all.shape)\n\n    if mismatched:\n        print(f\"âš ï¸ Column mismatch detected in {len(mismatched)} files (aligned to base columns).\")\n        # æƒ³çœ‹æ¸…å–®å¯ä»¥å–æ¶ˆä¸‹ä¸€è¡Œè¨»è§£\n        # print(\"\\n\".join(mismatched))\n\n    return df_all\n\nlandsat_raw_all = merge_landsat_batches(\n    out_dir=\"landsat_validation_raw_batches\",\n    merged_path=\"/tmp/landsat_validation_raw_all.parquet\"\n)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e404e6fd-89c7-48de-b062-678811457e07",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "type(landsat_val_raw.loc[10, \"green\"])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "f104ef32-7021-4d05-8ff0-1b0bb18e93c0",
      "metadata": {
        "language": "python",
        "name": "cell28"
      },
      "outputs": [],
      "source": "path = \"/tmp/landsat_validation_raw_all.parquet\"\nlandsat_val_raw = pd.read_parquet(path)\n\ndef compute_median_from_pixels(arr):\n    # median(skipna=True) + median==0 -> NaN\n    if arr is None:\n        return np.nan\n    a = np.array(arr, dtype=\"float\")\n    a = a[~np.isnan(a)]\n    if len(a) == 0:\n        return np.nan\n    median_val = float(np.median(a))\n    return median_val if median_val != 0 else np.nan\n\n# 0) å…ˆçœ‹ä¸€ä¸‹æœ‰å“ªäº›æ¬„ä½ï¼ˆdebug ç”¨ï¼‰\nprint(\"Columns:\", list(landsat_val_raw.columns))\n\n# 1) å»ºç«‹è¼¸å‡ºè¡¨ï¼šå…ˆæ”¾ key æ¬„ä½\nlandsat_val_features = landsat_val_raw[['Latitude', 'Longitude', 'Sample Date']].copy()\n\n# 2) è‡ªå‹•å°æ‡‰ band æ¬„ä½åç¨±ï¼ˆé¿å…ä½ æª”æ¡ˆè£¡ä¸æ˜¯ 'nir' è€Œæ˜¯ 'nir08' ä¹‹é¡ï¼‰\ncol_map_candidates = {\n    \"nir\":    [\"nir\", \"nir08\", \"B5\", \"SR_B5\"],\n    \"green\":  [\"green\", \"B3\", \"SR_B3\"],\n    \"swir16\": [\"swir16\", \"swir1\", \"B6\", \"SR_B6\"],\n    \"swir22\": [\"swir22\", \"swir2\", \"B7\", \"SR_B7\"],\n}\n\nresolved = {}\nfor out_name, candidates in col_map_candidates.items():\n    found = next((c for c in candidates if c in landsat_val_raw.columns), None)\n    resolved[out_name] = found\n\nmissing = [k for k, v in resolved.items() if v is None]\nif missing:\n    raise KeyError(\n        f\"These expected bands are missing in parquet: {missing}. \"\n        f\"Available columns: {list(landsat_val_raw.columns)}\"\n    )\n\nprint(\"Resolved band columns:\", resolved)\n\n# 3) median è¨ˆç®—ï¼šå¾ landsat_val_raw çš„åŸå§‹åƒç´ é™£åˆ—æ¬„ä½ç®—å‡ºä¾†ï¼Œå¯«å…¥ features\nfor out_name, raw_col in resolved.items():\n    landsat_val_features[out_name] = landsat_val_raw[raw_col].apply(compute_median_from_pixels)\n\n# 5) PUT åˆ° Snowflake workspace\nsession.sql(f\"\"\"\n    PUT file:///tmp/landsat_validation_raw_all.parquet\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")\n",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b130a364-1778-457e-ace5-b121dfcc7c7c",
      "metadata": {
        "name": "cell21",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "# Create indices: NDMI and MNDWI\neps = 1e-10\nlandsat_val_features['NDMI'] = (landsat_val_features['nir'] - landsat_val_features['swir16']) / (landsat_val_features['nir'] + landsat_val_features['swir16'])\nlandsat_val_features['MNDWI'] = (landsat_val_features['green'] - landsat_val_features['swir16']) / (landsat_val_features['green'] + landsat_val_features['swir16'] + eps)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dc7fe05a-2bfd-4d0f-a796-48a1e2f3b3c0",
      "metadata": {
        "name": "cell22",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "landsat_val_features['Latitude'] = Validation_df['Latitude']\nlandsat_val_features['Longitude'] = Validation_df['Longitude']\nlandsat_val_features['Sample Date'] = Validation_df['Sample Date']\nlandsat_val_features = landsat_val_features[['Latitude', 'Longitude', 'Sample Date', 'nir', 'green', 'swir16', 'swir22', 'NDMI', 'MNDWI']]"
    },
    {
      "cell_type": "code",
      "id": "4b7aa733-736c-4db7-840c-9e47e332348b",
      "metadata": {
        "language": "python",
        "name": "cell16",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "# Preview File\nlandsat_val_features.head()\nlandsat_val_features.to_csv(\"/tmp/landsat_features_validation_jin.csv\",index = False)",
      "execution_count": null
    },
    {
      "id": "d753f1e1-0346-4c46-a0fa-1441850aa35f",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "session.sql(\"\"\"\n    PUT file:///tmp/landsat_features_validation_jin.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8c287302-e015-4dff-b328-e8f5a639f9f8",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "**Note:** If you're using your own workspace, remember to replace \"EY-AI-and-Data-Challenge\" with your workspace name in the file path."
    },
    {
      "id": "f396bb98-69f9-4893-b545-f3c873cb45ee",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "",
      "outputs": [],
      "execution_count": null
    }
  ]
}