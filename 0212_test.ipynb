{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec642413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and feature datasets\n",
    "train_water = pd.read_csv('/home/oai/share/water_quality_training_dataset.csv')\n",
    "train_landsat = pd.read_csv('/home/oai/share/landsat_features_training_jin.csv')\n",
    "train_terra = pd.read_csv('/home/oai/share/terraclimate_features_training.csv')\n",
    "\n",
    "# Load validation feature datasets\n",
    "val_landsat = pd.read_csv('/home/oai/share/landsat_features_validation_jin.csv')\n",
    "val_terra = pd.read_csv('/home/oai/share/terraclimate_features_validation.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge landsat and terraclimate features for training\n",
    "train_features = pd.merge(train_landsat, train_terra, on=['Latitude','Longitude','Sample Date'], how='inner')\n",
    "\n",
    "# Merge merged features with water quality training data\n",
    "train_full = pd.merge(train_water, train_features, on=['Latitude','Longitude','Sample Date'], how='inner')\n",
    "\n",
    "print('Training data shape after merge:', train_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sample Date to datetime\n",
    "train_full['Sample Date'] = pd.to_datetime(train_full['Sample Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "# Extract date-related features\n",
    "train_full['date_ordinal'] = train_full['Sample Date'].map(pd.Timestamp.toordinal)\n",
    "train_full['year'] = train_full['Sample Date'].dt.year\n",
    "train_full['month'] = train_full['Sample Date'].dt.month\n",
    "train_full['dayofyear'] = train_full['Sample Date'].dt.dayofyear\n",
    "\n",
    "# Prepare feature columns (exclude targets and original date)\n",
    "target_cols = ['Total Alkalinity','Electrical Conductance','Dissolved Reactive Phosphorus']\n",
    "feature_cols = [col for col in train_full.columns if col not in target_cols + ['Sample Date']]\n",
    "\n",
    "# Split into features (X) and targets (y)\n",
    "X_train = train_full[feature_cols]\n",
    "y_train = train_full[target_cols]\n",
    "\n",
    "# Fill missing values in training features with median values\n",
    "median_vals = X_train.median()\n",
    "X_train_filled = X_train.fillna(median_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb87dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XGBoost parameters\n",
    "xgb_params = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "# Initialize and train MultiOutputRegressor with XGBRegressor\n",
    "base_model = XGBRegressor(**xgb_params)\n",
    "model = MultiOutputRegressor(base_model)\n",
    "model.fit(X_train_filled, y_train)\n",
    "\n",
    "# Compute feature importances by averaging across the individual estimators\n",
    "importances = np.mean([est.feature_importances_ for est in model.estimators_], axis=0)\n",
    "feature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Save feature importances to CSV\n",
    "feature_importance_df.to_csv('/home/oai/share/feature_importance.csv', index=False)\n",
    "\n",
    "# Display top 10 features\n",
    "feature_importance_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge validation landsat and terraclimate features\n",
    "val_features = pd.merge(val_landsat, val_terra, on=['Latitude','Longitude','Sample Date'], how='inner')\n",
    "\n",
    "# Convert Sample Date to datetime for validation\n",
    "val_features['Sample Date'] = pd.to_datetime(val_features['Sample Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "# Create date features for validation\n",
    "val_features['date_ordinal'] = val_features['Sample Date'].map(pd.Timestamp.toordinal)\n",
    "val_features['year'] = val_features['Sample Date'].dt.year\n",
    "val_features['month'] = val_features['Sample Date'].dt.month\n",
    "val_features['dayofyear'] = val_features['Sample Date'].dt.dayofyear\n",
    "\n",
    "# Select the same feature columns as training\n",
    "X_val = val_features[feature_cols]\n",
    "\n",
    "# Fill missing values in validation features using training medians\n",
    "X_val_filled = X_val.fillna(median_vals)\n",
    "\n",
    "# Make predictions\n",
    "preds = model.predict(X_val_filled)\n",
    "\n",
    "# Load submission template and fill predictions\n",
    "submission = pd.read_csv('/home/oai/share/submission_template.csv')\n",
    "submission[target_cols] = preds\n",
    "\n",
    "# Save completed submission\n",
    "submission.to_csv('/home/oai/share/submission.csv', index=False)\n",
    "\n",
    "# Display first few rows of the submission\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
