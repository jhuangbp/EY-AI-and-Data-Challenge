{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec642413",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import MultiOutputRegressor"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad32d0bb",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Load training and feature datasets\ntrain_water = pd.read_csv('water_quality_training_dataset.csv')\ntrain_landsat = pd.read_csv('landsat_features_training_jin2.csv')\ntrain_terra = pd.read_csv('terraclimate_features_training_full.csv')\n\n# Load validation feature datasets\nval_landsat = pd.read_csv('landsat_features_validation_jin2.csv')\nval_terra = pd.read_csv('terraclimate_features_validation_full.csv')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c53145d",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Merge landsat and terraclimate features for training\ntrain_features = pd.merge(train_landsat, train_terra, on=['Latitude','Longitude','Sample Date'], how='inner')\n\n# Merge merged features with water quality training data\ntrain_full = pd.merge(train_water, train_features, on=['Latitude','Longitude','Sample Date'], how='inner')\n\nprint('Training data shape after merge:', train_full.shape)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c683466f",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Convert Sample Date to datetime\ntrain_full['Sample Date'] = pd.to_datetime(train_full['Sample Date'], format='%d-%m-%Y', errors='coerce')\n\n# Extract date-related features\ntrain_full['date_ordinal'] = train_full['Sample Date'].map(pd.Timestamp.toordinal)\ntrain_full['year'] = train_full['Sample Date'].dt.year\ntrain_full['month'] = train_full['Sample Date'].dt.month\ntrain_full['dayofyear'] = train_full['Sample Date'].dt.dayofyear\n\n# Prepare feature columns (exclude targets and original date)\ntarget_cols = ['Total Alkalinity','Electrical Conductance','Dissolved Reactive Phosphorus']\nfeature_cols = [col for col in train_full.columns if col not in target_cols + ['Sample Date', 'Latitude', 'Longitude', 'date_ordinal', 'year']]\n\n# Split into features (X) and targets (y)\nX_train = train_full[feature_cols]\ny_train = train_full[target_cols]\n\n# Fill missing values in training features with median values\nmedian_vals = X_train.median()\nX_train_filled = X_train.fillna(median_vals)"
    },
    {
      "id": "806c577c-afbb-45de-9b05-e52134bb66d1",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "feature_cols",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb87dcc",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Define XGBoost parameters\nxgb_params = {\n    'n_estimators': 300,\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': 42,\n    'objective': 'reg:squarederror'\n}\n\n# Initialize and train MultiOutputRegressor with XGBRegressor\nbase_model = XGBRegressor(**xgb_params)\nmodel = MultiOutputRegressor(base_model)\nmodel.fit(X_train_filled, y_train)\n\n# Compute feature importances by averaging across the individual estimators\nimportances = np.mean([est.feature_importances_ for est in model.estimators_], axis=0)\nfeature_importance_df = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n\n# Save feature importances to CSV\nfeature_importance_df.to_csv('feature_importance.csv', index=False)\n\n# Display top 10 features\nfeature_importance_df.head(10)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41c717a",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": "# Merge validation landsat and terraclimate features\nval_features = pd.merge(val_landsat, val_terra, on=['Latitude','Longitude','Sample Date'], how='inner')\n\n# Convert Sample Date to datetime for validation\nval_features['Sample Date'] = pd.to_datetime(val_features['Sample Date'], format='%d-%m-%Y', errors='coerce')\n\n# Create date features for validation\nval_features['date_ordinal'] = val_features['Sample Date'].map(pd.Timestamp.toordinal)\nval_features['year'] = val_features['Sample Date'].dt.year\nval_features['month'] = val_features['Sample Date'].dt.month\nval_features['dayofyear'] = val_features['Sample Date'].dt.dayofyear\n\n# Select the same feature columns as training\nX_val = val_features[feature_cols]\n\n# Fill missing values in validation features using training medians\nX_val_filled = X_val.fillna(median_vals)\n\n# Make predictions\npreds = model.predict(X_val_filled)\n\n# Load submission template and fill predictions\nsubmission = pd.read_csv('submission_template.csv')\nsubmission[target_cols] = preds\n\n# Save completed submission\nsubmission.to_csv('submission.csv', index=False)\n\n# Display first few rows of the submission\nsubmission.head()"
    },
    {
      "id": "c8007edb-982a-4e7c-9416-e1e614c99fb5",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "import snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n\nsubmission.to_csv(\"/tmp/submission_v6.csv\",index = False)\n\nsession.sql(\"\"\"\n    PUT file:///tmp/submission_v6.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")\n\n",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}