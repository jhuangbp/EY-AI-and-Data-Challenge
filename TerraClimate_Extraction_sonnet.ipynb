{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "lastEditStatus": {
      "notebookId": "y3vizltueoic3nbogu5h",
      "authorId": "8166278287717",
      "authorName": "DATACHALLENGE",
      "authorEmail": "datachallenge@ey.com",
      "sessionId": "e748d7dd-17f5-41c2-8af3-9fbc693bd759",
      "lastEditTime": 1765775413840
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "616441fc-62fd-43c9-961d-5b4606d02a3a",
      "metadata": {
        "name": "cell1",
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": "## 2026 EY AI & Data Challenge - TerraClimate Data Extraction Notebook\n\nThis notebooks demonstrates how to access the TerraClimate dataset. TerraClimate is a dataset of monthly climate and climatic water balance for global terrestrial surfaces from 1958 to the present. These data provide important inputs for ecological and hydrological studies at global scales that require high spatial resolution and time-varying data. All data have monthly temporal resolution and a ~4-km (1/24th degree) spatial resolution. This dataset is provided in Zarr format. \n\nFor more information, visit: [terraclimate- overview](https://planetarycomputer.microsoft.com/dataset/terraclimate#overview) "
    },
    {
      "id": "c8dd7a3f-872e-4e04-8267-5d2c1ef4bcf4",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Load In Dependencies\nThe following code installs the required Python libraries (found in the requirements.txt file) in the Snowflake environment to allow successful execution of the remaining notebook code. After running this code for the first time, it is required to â€œrestartâ€ the kernal so the Python libraries are available in the environment. This is done by selecting the â€œConnectedâ€ menu above the notebook (next to â€œRun allâ€) and selecting the â€œrestart kernalâ€ link. Subsequent runs of the notebook do not require this â€œrestartâ€ process."
    },
    {
      "cell_type": "code",
      "id": "ba9c9092-a1e5-410b-95cd-568e8bcca686",
      "metadata": {
        "language": "python",
        "name": "cell3"
      },
      "outputs": [],
      "source": "!pip install uv\n!uv pip install  -r requirements.txt ",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "35f9d94d-ecc7-45bf-812d-05d62f3b42d3",
      "metadata": {
        "name": "cell2",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "import snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\n\n# Multi-dimensional arrays and datasets (e.g., NetCDF, Zarr)\nimport xarray as xr\n\nfrom scipy.spatial import cKDTree\n\n# Planetary Computer tools for STAC API access and authentication\nimport pystac_client\nimport planetary_computer as pc\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os"
    },
    {
      "cell_type": "markdown",
      "id": "a74652f9-d132-4bd2-b444-beabc82fbeb0",
      "metadata": {
        "name": "cell20",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Extracting TerraClimate Data Using API Calls\n\nThe API-based method allows us to efficiently access **TerraClimate** data for specific regions and time periods through the [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/), ensuring scalability and reproducibility of the process.\n\nThrough the API, we can extract climate variables such as **Potential Evapotranspiration (PET)**, which represents the atmospheric demand for water. This variable provides important insights into surface moisture balance and helps improve the accuracy of water quality modeling.\n\nThis approach ensures consistent, automated retrieval of high-resolution climate data that can be easily integrated with satellite-derived features for comprehensive environmental and hydrological analysis.\n"
    },
    {
      "cell_type": "markdown",
      "id": "33d42366-3ab5-47b4-bf86-32bb9c63eb7a",
      "metadata": {
        "name": "cell4",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Loading and Mapping TerraClimate Data\n\nThis section demonstrates how **TerraClimate climate variables**, such as **Potential Evapotranspiration (PET)**, are loaded and mapped to sampling locations.\n\n- The **load_terraclimate_dataset** function opens the TerraClimate Zarr/NetCDF dataset from the Microsoft Planetary Computer, handling storage options automatically.\n- The **filterg** function filters the dataset for the desired time range (2011â€“2015) and the spatial extent corresponding to the study region. The resulting data is converted into a pandas DataFrame with standardized column names.\n- The **assign_nearest_climate** function maps each sampling location to its **nearest TerraClimate grid point** using a KD-tree and assigns the climate variable values corresponding to the closest timestamp.\n\nThis workflow ensures efficient, reproducible retrieval of climate variables, while allowing participants to work with pre-extracted CSV files for faster benchmarking and analysis.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ad324597-e798-4127-8668-da9ab6873467",
      "metadata": {
        "name": "cell5",
        "language": "python"
      },
      "outputs": [],
      "source": "def load_terraclimate_dataset():\n    catalog = pystac_client.Client.open(\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n        modifier=pc.sign_inplace,\n    )\n    collection = catalog.get_collection(\"terraclimate\")\n    asset = collection.assets[\"zarr-abfs\"]\n\n    if \"xarray:storage_options\" in asset.extra_fields:\n        ds = xr.open_zarr(\n            asset.href,\n            storage_options=asset.extra_fields[\"xarray:storage_options\"],\n            consolidated=True,\n        )\n    else:\n        ds = xr.open_dataset(\n            asset.href,\n            **asset.extra_fields[\"xarray:open_kwargs\"],\n        )\n\n    return ds"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c8cd1bc1-0c50-4fee-926c-459927f397e2",
      "metadata": {
        "name": "cell6",
        "language": "python"
      },
      "outputs": [],
      "source": "# --- Filtering function (kept identical) ---\n# def filterg(ds, var):\n#     ds_2011_2015 = ds[var].sel(time=slice(\"2011-01-01\", \"2015-12-31\"))\n\n#     df_var_append = []\n#     for i in tqdm(range(len(ds_2011_2015.time))):\n#         df_var = ds_2011_2015.isel(time=i).to_dataframe().reset_index()\n#         df_var_filter = df_var[\n#             (df_var['lat'] > -35.18) & (df_var['lat'] < -21.72) &\n#             (df_var['lon'] > 14.97) & (df_var['lon'] < 32.79)\n#         ]\n#         df_var_append.append(df_var_filter)\n\n#     df_var_final = pd.concat(df_var_append, ignore_index=True)\n#     print(f\"Filtering for {var} completed\")\n\n#     df_var_final['time'] = df_var_final['time'].astype(str)\n\n#     # Column mapping\n#     col_mapping = {\"lat\": \"Latitude\", \"lon\": \"Longitude\", \"time\": \"Sample Date\"}\n#     df_var_final = df_var_final.rename(columns=col_mapping)\n\n#     return df_var_final\ndef filterg(ds, var):\n    da = ds[var].sel(time=slice(\"2011-01-01\", \"2015-12-31\"))\n\n    # bbox filterï¼ˆè·Ÿä½ åŽŸæœ¬ä¸€è‡´ï¼‰\n    da = da.where(\n        (da[\"lat\"] > -35.18) & (da[\"lat\"] < -21.72) &\n        (da[\"lon\"] > 14.97)  & (da[\"lon\"] < 32.79),\n        drop=True\n    )\n\n    df = da.to_dataframe().reset_index()\n    df[\"time\"] = df[\"time\"].astype(str)\n\n    df = df.rename(columns={\"lat\": \"Latitude\", \"lon\": \"Longitude\", \"time\": \"Sample Date\"})\n    print(f\"Filtering for {var} completed, rows={len(df)}\")\n    return df\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c1a70cf-cfdb-4256-808d-087d591e806f",
      "metadata": {
        "name": "cell7",
        "language": "python"
      },
      "outputs": [],
      "source": "# --- Climate variable assignment function (unchanged logic) ---\ndef assign_nearest_climate(sa_df, climate_df, var_name):\n    \"\"\"\n    Map nearest climate variable values to a new DataFrame \n    containing only the specified variable column.\n    \"\"\"\n    sa_coords = np.radians(sa_df[['Latitude', 'Longitude']].values)\n    climate_coords = np.radians(climate_df[['Latitude', 'Longitude']].values)\n\n    tree = cKDTree(climate_coords)\n    dist, idx = tree.query(sa_coords, k=1)\n\n    nearest_points = climate_df.iloc[idx].reset_index(drop=True)\n\n    sa_df = sa_df.reset_index(drop=True)\n    sa_df[['nearest_lat', 'nearest_lon']] = nearest_points[['Latitude', 'Longitude']]\n\n    sa_df['Sample Date'] = pd.to_datetime(sa_df['Sample Date'], dayfirst=True, errors='coerce')\n    climate_df['Sample Date'] = pd.to_datetime(climate_df['Sample Date'], dayfirst=True, errors='coerce')\n\n    climate_values = []\n\n    for i in tqdm(range(len(sa_df)), desc=f\"Mapping {var_name.upper()} values\"):\n        sample_date = sa_df.loc[i, 'Sample Date']\n        nearest_lat = sa_df.loc[i, 'nearest_lat']\n        nearest_lon = sa_df.loc[i, 'nearest_lon']\n\n        subset = climate_df[\n            (climate_df['Latitude'] == nearest_lat) &\n            (climate_df['Longitude'] == nearest_lon)\n        ]\n\n        if subset.empty:\n            climate_values.append(np.nan)\n            continue\n\n        nearest_idx = (subset['Sample Date'] - sample_date).abs().idxmin()\n        climate_values.append(subset.loc[nearest_idx, var_name])\n\n    output_df = pd.DataFrame({var_name: climate_values})\n\n    \n    return output_df"
    },
    {
      "cell_type": "markdown",
      "id": "8065c081-51ca-439e-b42d-fa1afcb961e9",
      "metadata": {
        "name": "cell8",
        "codeCollapsed": true
      },
      "source": "### Extracting features for the training dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "54291642-9b6a-42a5-8370-12c230b755d5",
      "metadata": {
        "name": "cell9",
        "language": "python"
      },
      "outputs": [],
      "source": "Water_Quality_df = pd.read_csv(\"water_quality_training_dataset.csv\")\ndisplay(Water_Quality_df.head(5))"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "edd9dbc3-56c7-46be-b921-666d55a50185",
      "metadata": {
        "name": "cell10",
        "language": "python"
      },
      "outputs": [],
      "source": "Water_Quality_df.shape"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "34229a57-1579-4615-8b07-64fde8299e3e",
      "metadata": {
        "name": "cell11",
        "language": "python"
      },
      "outputs": [],
      "source": "# # Load TerraClimate dataset, filter (time,region,parameter), filter for nearest parameter values\n# ds = load_terraclimate_dataset()\n# tc_parameter = filterg(ds,'pet')\n# Terraclimate_training_df = assign_nearest_climate(Water_Quality_df, tc_parameter, 'pet')\n# Load TerraClimate dataset\nds = load_terraclimate_dataset()\n\n# æƒ³æŠ½çš„è®Šæ•¸ï¼ˆç…§ä½ æˆªåœ–ï¼‰\ntc_vars = [\n    \"aet\", \"def\", \"pdsi\", \"pet\", \"ppt\",\n    \"q\", \"soil\", \"srad\", \"swe\",\n    \"tmax\", \"tmin\",\n    \"vap\", \"vpd\", \"ws\",\n    \"ppt_station_influence\", \"tmax_station_influence\", \"tmin_station_influence\", \"vap_station_influence\"\n]\n\n# æª¢æŸ¥ dataset çœŸçš„æœ‰å“ªäº›è®Šæ•¸\navailable = set(ds.data_vars)\ntc_vars_exist = [v for v in tc_vars if v in available]\ntc_vars_missing = [v for v in tc_vars if v not in available]\n\nprint(\"Will extract:\", tc_vars_exist)\nprint(\"Missing in dataset:\", tc_vars_missing)\n\n# é€ä¸€æŠ½å–ä¸¦å°æ‡‰åˆ°æ¯ç­† Water_Quality_df\nTerraclimate_training_df = Water_Quality_df[[\"Latitude\", \"Longitude\", \"Sample Date\"]].copy()\n\nfor i, v in enumerate(tc_vars_exist):\n    if i % 3 == 0:                      # æ¯ 3 å€‹è®Šæ•¸ refresh token\n        ds = load_terraclimate_dataset()\n    tc_parameter = filterg(ds, v)\n    Terraclimate_training_df[v] = assign_nearest_climate(Water_Quality_df, tc_parameter, v)[v]"
    },
    {
      "cell_type": "markdown",
      "id": "fe-markdown-001",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Feature Engineering: æ•¸å€¼è½‰æ›ã€äº¤äº’é … & æ°£å€™è·å¹³\n\næ­¤å€å¡Šé‡å°å·²æŠ“å–çš„ TerraClimate è®Šæ•¸é€²è¡Œä¸‰é¡žç‰¹å¾µå·¥ç¨‹ï¼Œåˆ†åˆ¥å°æ‡‰ `add_basic_derived`ã€`add_interactions`ã€`add_anomalies` ä¸‰å€‹å‡½æ•¸ï¼š\n\n**ä¸€ã€æ•¸å€¼è½‰æ›ï¼ˆ`add_basic_derived`ï¼‰**\n- Log1p è½‰æ›ï¼šå°å³ååˆ†å¸ƒçš„æ°´æ–‡è®Šæ•¸ï¼ˆppt, q, pet, aetï¼‰å– log(1+x)ï¼Œé™ä½Žæ¥µç«¯å€¼å½±éŸ¿\n- æ—¥å¤œæº«å·® DTRï¼štmax âˆ’ tminï¼Œåæ˜ è’¸ç™¼èˆ‡æ°´ä¸­æº¶æ°§å‹•æ…‹\n- æ°´åˆ†è™§ç¼ºæ¯”ä¾‹ï¼ˆwater_stress_ratioï¼‰ï¼šdef / petï¼Œæ¯” def çµ•å°å€¼æ›´ç©©å®šçš„ä¹¾æ—±ä»£ç†æŒ‡æ¨™\n\n**äºŒã€äº¤äº’é …ï¼ˆ`add_interactions`ï¼‰**\n- `aet_pet_ratio`ï¼šå¯¦éš›/æ½›åœ¨è’¸æ•£æ¯” â†’ åœ°è¡¨ä¹¾æ¿•ç¨‹åº¦\n- `ppt_x_q`ï¼šé™é›¨ Ã— é€•æµ â†’ æ²–åˆ·æ±¡æŸ“æ•ˆæ‡‰ï¼ˆæ¿åº¦ã€å¤§è…¸æ¡¿èŒç­‰ï¼‰\n- `tmax_x_vpd`ï¼šæº«åº¦ Ã— è’¸æ°£å£“å·® â†’ è—»é¡žç”Ÿé•·é¢¨éšªï¼ˆæ‚¶ç†±æ¢ä»¶ï¼‰\n- `soil_x_ppt`ï¼šåœŸå£¤æ°´åˆ† Ã— é™é›¨ â†’ è¾²æ¥­é¢æºæ±¡æŸ“ï¼ˆåœŸå£¤é£½å’Œæ™‚å¾‘æµå¸¶èµ°æ±¡æŸ“ç‰©ï¼‰\n- `runoff_ratio`ï¼šé€•æµ/é™é›¨æ¯” â†’ é›†æ°´å€ç”¢æµèƒ½åŠ›\n- `srad_x_tmax`ï¼šå¤ªé™½è¼»å°„ Ã— æœ€é«˜æº« â†’ æ°´é«”å„ªé¤ŠåŒ–ä»£ç†æŒ‡æ¨™\n\n**å››ã€æ°£å€™è·å¹³ï¼ˆ`add_anomalies`ï¼‰**\n- å°æ¯å€‹æŽ¡æ¨£é»žã€æ¯å€‹æœˆä»½ï¼Œè¨ˆç®—è§€æ¸¬å€¼èˆ‡æ­·å²åŒæœˆï¼ˆå…¨è³‡æ–™é›† 1958â€“presentï¼‰å‡å€¼çš„å·®è·\n- è·å¹³ = ç•¶æœˆå€¼ âˆ’ è©²æ ¼é»žæ­·å²åŒæœˆå¹³å‡ï¼Œæ•¸å€¼ä»£è¡¨ã€Œåé›¢æ­£å¸¸æ°£å€™ã€çš„ç¨‹åº¦\n- é©ç”¨è®Šæ•¸ï¼šppt, pet, aet, def, pdsi, q, soil, tmax, tmin, vpd, srad\n- è·å¹³æ¯”çµ•å°å€¼æ›´èƒ½æ•æ‰ç•°å¸¸æ°£å€™äº‹ä»¶å°æ°´è³ªçš„è¡æ“Šï¼ˆä¾‹å¦‚è¶…ä¹¾æ—±æœˆã€æ¥µç«¯æš´é›¨ï¼‰\n"
    },
    {
      "cell_type": "code",
      "id": "fe-code-001",
      "metadata": {
        "language": "python",
        "name": "cell_fe"
      },
      "outputs": [],
      "execution_count": null,
      "source": "# ===========================================================\n# Feature Engineering å‡½æ•¸å®šç¾©\n# ===========================================================\n\ndef add_basic_derived(df):\n    \"\"\"\n    ä¸€ã€æ•¸å€¼è½‰æ›ï¼ˆå–®è®Šæ•¸ï¼‰\n      - log1p è½‰æ›ï¼šppt, q, pet, aet\n      - æ—¥å¤œæº«å·® DTR\n      - æ°´åˆ†è™§ç¼ºæ¯”ä¾‹ water_stress_ratio\n    \"\"\"\n    df = df.copy()\n\n    for col in ['ppt', 'q', 'pet', 'aet']:\n        if col in df.columns:\n            df[f'log1p_{col}'] = np.log1p(df[col].clip(lower=0))\n\n    if 'tmax' in df.columns and 'tmin' in df.columns:\n        df['dtr'] = df['tmax'] - df['tmin']\n\n    if 'def' in df.columns and 'pet' in df.columns:\n        df['water_stress_ratio'] = df['def'] / (df['pet'] + 1e-6)\n\n    print(f\"[add_basic_derived] å®Œæˆï¼Œç›®å‰æ¬„æ•¸ï¼š{df.shape[1]}\")\n    return df\n\n\ndef add_interactions(df):\n    \"\"\"\n    äºŒã€äº¤äº’é …ï¼ˆæ°´æ–‡ç”Ÿæ…‹æ„ç¾©æ˜Žç¢ºçš„çµ„åˆç‰¹å¾µï¼‰\n      - aet_pet_ratio, ppt_x_q, tmax_x_vpd\n      - soil_x_ppt, runoff_ratio, srad_x_tmax\n    \"\"\"\n    df = df.copy()\n\n    if 'aet' in df.columns and 'pet' in df.columns:\n        df['aet_pet_ratio'] = df['aet'] / (df['pet'] + 1e-6)\n\n    if 'ppt' in df.columns and 'q' in df.columns:\n        df['ppt_x_q'] = df['ppt'] * df['q']\n\n    if 'tmax' in df.columns and 'vpd' in df.columns:\n        df['tmax_x_vpd'] = df['tmax'] * df['vpd']\n\n    if 'soil' in df.columns and 'ppt' in df.columns:\n        df['soil_x_ppt'] = df['soil'] * df['ppt']\n\n    if 'q' in df.columns and 'ppt' in df.columns:\n        df['runoff_ratio'] = df['q'] / (df['ppt'] + 1e-6)\n\n    if 'srad' in df.columns and 'tmax' in df.columns:\n        df['srad_x_tmax'] = df['srad'] * df['tmax']\n\n    print(f\"[add_interactions] å®Œæˆï¼Œç›®å‰æ¬„æ•¸ï¼š{df.shape[1]}\")\n    return df\n\n\ndef add_anomalies(df, ds, vars_list):\n    \"\"\"\n    å››ã€æ°£å€™è·å¹³ç‰¹å¾µ\n    å°æ¯å€‹æŽ¡æ¨£é»žçš„æ¯å€‹è®Šæ•¸ï¼Œè¨ˆç®—ï¼š\n        è·å¹³ = ç•¶æœˆå€¼ âˆ’ è©²æ ¼é»žæ­·å²åŒæœˆï¼ˆå…¨å¹´ä»½ï¼‰å‡å€¼\n    \n    Parameters\n    ----------\n    df       : pd.DataFrameï¼Œå« Latitude, Longitude, Sample Date åŠå·²æŠ“å–çš„æ°£å€™æ¬„ä½\n    ds       : xarray.Datasetï¼ŒTerraClimate å®Œæ•´è³‡æ–™é›†ï¼ˆç”± load_terraclimate_dataset() å–å¾—ï¼‰\n    vars_list: list[str]ï¼Œè¦è¨ˆç®—è·å¹³çš„è®Šæ•¸åç¨±æ¸…å–®\n    \n    Returns\n    -------\n    pd.DataFrameï¼ŒåŽŸæ¬„ä½ + å„è®Šæ•¸çš„ _anom æ¬„ä½\n    \"\"\"\n    df = df.copy()\n    df['_month'] = pd.to_datetime(df['Sample Date'], dayfirst=True, errors='coerce').dt.month\n\n    # è¨ˆç®—æ¯å€‹æ ¼é»ž Ã— æ¯æœˆçš„æ­·å²å‡å€¼ï¼ˆclimatologyï¼‰\n    # ä½¿ç”¨ ds å…¨å¹´ä»½è³‡æ–™ï¼Œgroupby month å–å‡å€¼\n    for var in vars_list:\n        if var not in df.columns:\n            print(f\"  [skip] {var} ä¸åœ¨ df ä¸­ï¼Œç•¥éŽ\")\n            continue\n        if var not in ds.data_vars:\n            print(f\"  [skip] {var} ä¸åœ¨ ds ä¸­ï¼Œç•¥éŽ\")\n            continue\n\n        print(f\"  è¨ˆç®— {var} è·å¹³...\")\n\n        # å¾ž ds è¨ˆç®—æ­·å²æœˆå‡å€¼ï¼ˆæ‰€æœ‰å¹´ä»½ï¼ŒæŒ‰æœˆä»½ groupbyï¼‰\n        da = ds[var]\n        # ä¾æœˆä»½åˆ†çµ„å¾Œå¹³å‡ â†’ shape: (12, lat, lon)\n        clim = da.groupby('time.month').mean('time')\n\n        # è½‰æˆ DataFrame ä¾¿æ–¼æŸ¥è¡¨\n        clim_df = clim.to_dataframe().reset_index()\n        clim_df = clim_df.rename(columns={'lat': 'Latitude', 'lon': 'Longitude', var: f'{var}_clim'})\n\n        # å°æ¯ç­†æ¨£æœ¬æŸ¥æ‰¾å°æ‡‰æ ¼é»ž Ã— æœˆä»½çš„æ­·å²å‡å€¼\n        anom_vals = []\n        lats = df['Latitude'].values\n        lons = df['Longitude'].values\n        months = df['_month'].values\n        obs_vals = df[var].values\n\n        # å»º KD-treeï¼ˆåªéœ€å»ºä¸€æ¬¡ï¼‰\n        clim_coords = np.radians(clim_df[['Latitude', 'Longitude']].values)\n        tree = cKDTree(clim_coords)\n\n        for i in range(len(df)):\n            sample_coord = np.radians([[lats[i], lons[i]]])\n            _, idx = tree.query(sample_coord, k=1)\n            month = months[i]\n\n            clim_row = clim_df.iloc[idx[0]]\n            # è‹¥è©²ç­† clim_df æ˜¯å¤šæœˆçš„ï¼ˆæœˆä»½å·²å±•é–‹ï¼‰ï¼Œéœ€å† filter month\n            # å…ˆå–å°æ‡‰ month çš„ clim å€¼\n            clim_month_val = clim_df[\n                (np.abs(clim_df['Latitude'] - lats[i]) < 0.05) &\n                (np.abs(clim_df['Longitude'] - lons[i]) < 0.05) &\n                (clim_df['month'] == month)\n            ][f'{var}_clim']\n\n            if len(clim_month_val) == 0:\n                anom_vals.append(np.nan)\n            else:\n                anom_vals.append(obs_vals[i] - clim_month_val.values[0])\n\n        df[f'{var}_anom'] = anom_vals\n\n    df = df.drop(columns=['_month'])\n    anom_cols = [c for c in df.columns if c.endswith('_anom')]\n    print(f\"[add_anomalies] å®Œæˆï¼Œæ–°å¢ž {len(anom_cols)} å€‹è·å¹³æ¬„ä½ï¼š{anom_cols}\")\n    return df\n\n\n# å‘ä¸‹ç›¸å®¹èˆŠç‰ˆå‘¼å«ï¼ˆåˆä½µä¸‰æ­¥é©Ÿï¼‰\ndef apply_feature_engineering(df):\n    df = add_basic_derived(df)\n    df = add_interactions(df)\n    return df\n"
    },
    {
      "cell_type": "code",
      "id": "fe-code-002",
      "metadata": {
        "language": "python",
        "name": "cell_fe_apply_training"
      },
      "outputs": [],
      "execution_count": null,
      "source": "print(\"=== å° Training è³‡æ–™é€²è¡Œ 1+2+4 ç‰¹å¾µå·¥ç¨‹ ===\")\nTerraclimate_training_df = add_basic_derived(Terraclimate_training_df)\nTerraclimate_training_df = add_interactions(Terraclimate_training_df)\n\nvars_for_anom = ['ppt', 'pet', 'aet', 'def', 'pdsi', 'q', 'soil', 'tmax', 'tmin', 'vpd', 'srad']\nTerraclimate_training_df = add_anomalies(Terraclimate_training_df, ds, vars_for_anom)\n\nprint(f\"\\nâœ… Training å®Œæˆï¼shape: {Terraclimate_training_df.shape}\")\nTerraclimate_training_df.to_csv(\"terraclimate_features_training_sonnet_v2.csv\", index=False)\ndisplay(Terraclimate_training_df.head())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4c2eebd3-78ab-4306-ba51-57542dee97bc",
      "metadata": {
        "name": "cell12",
        "language": "python"
      },
      "outputs": [],
      "source": "# Terraclimate_training_df['Latitude'] = Water_Quality_df['Latitude']\n# Terraclimate_training_df['Longitude'] = Water_Quality_df['Longitude']\n# Terraclimate_training_df['Sample Date'] = Water_Quality_df['Sample Date']\n# Terraclimate_training_df = Terraclimate_training_df[['Latitude', 'Longitude', 'Sample Date', 'pet']]\n# Terraclimate_training_df.to_csv('terraclimate_features_training.csv', index=False)\nTerraclimate_training_df.to_csv(\"terraclimate_features_training_sonnet_v2.csv\", index=False)\nprint(\"Saved:\", Terraclimate_training_df.shape)\nTerraclimate_training_df.head()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2c2114e6-8204-4cf7-8f0d-16745cb1fd75",
      "metadata": {
        "name": "cell13",
        "language": "python",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "# Preview File\ndisplay(Terraclimate_training_df.head())"
    },
    {
      "id": "3e2552f2-b2ec-40d2-816f-43293c78c9f6",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "Terraclimate_training_df.to_csv(\"/tmp/terraclimate_features_training_sonnet_v2.csv\",index = False)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d45a095d-f7fd-4418-a932-408dd8cd86bd",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "session.sql(\"\"\"\n    PUT file:///tmp/terraclimate_features_training_sonnet_v2.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c6f9eb77-71be-45ad-9738-b324f92c53a6",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "**Note:** If you're using your own workspace, remember to replace \"EY-AI-and-Data-Challenge\" with your workspace name in the file path."
    },
    {
      "cell_type": "markdown",
      "id": "b91cf806-2228-4b46-a0c1-8b1afb01bc6d",
      "metadata": {
        "name": "cell14",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "### Extracting features for the validation dataset"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9ed3f63d-3a89-4145-b2a6-917c2e98fa4e",
      "metadata": {
        "name": "cell15",
        "language": "python"
      },
      "outputs": [],
      "source": "Validation_df=pd.read_csv('submission_template.csv')\ndisplay(Validation_df.head())"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b32f7a00-313d-4acf-8a02-2d755703da27",
      "metadata": {
        "name": "cell16",
        "language": "python"
      },
      "outputs": [],
      "source": "Validation_df.shape"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d4a6c090-39df-4923-b03b-1fc257e2952b",
      "metadata": {
        "name": "cell17",
        "language": "python"
      },
      "outputs": [],
      "source": "# # Load TerraClimate dataset, filter (time,region,parameter), filter for nearest parameter values\n# Terraclimate_validation_df = assign_nearest_climate(Validation_df, tc_parameter, 'pet')\n# ç¢ºä¿ ds èˆ‡ tc_vars_exist å·²åœ¨å‰é¢ cell11 å»ºå¥½\n# ds = load_terraclimate_dataset()\n# tc_vars_exist = [...]\n\nTerraclimate_validation_df = Validation_df[[\"Latitude\", \"Longitude\", \"Sample Date\"]].copy()\n\nfor v in tc_vars_exist:\n    tc_parameter = filterg(ds, v)\n    Terraclimate_validation_df[v] = assign_nearest_climate(Validation_df, tc_parameter, v)[v]\n\n# å›ºå®šæ¬„ä½é †åºï¼ˆå¯ç•™å¯ä¸ç•™ï¼‰\nTerraclimate_validation_df = Terraclimate_validation_df[[\"Latitude\", \"Longitude\", \"Sample Date\"] + tc_vars_exist]\n"
    },
    {
      "cell_type": "code",
      "id": "fe-code-003",
      "metadata": {
        "language": "python",
        "name": "cell_fe_apply_validation"
      },
      "outputs": [],
      "execution_count": null,
      "source": "print(\"=== å° Validation è³‡æ–™é€²è¡Œç›¸åŒè™•ç† ===\")\nTerraclimate_validation_df = add_basic_derived(Terraclimate_validation_df)\nTerraclimate_validation_df = add_interactions(Terraclimate_validation_df)\nTerraclimate_validation_df = add_anomalies(Terraclimate_validation_df, ds, vars_for_anom)\n\nprint(f\"\\nðŸŽ‰ å®Œæˆï¼ Training shape: {Terraclimate_training_df.shape} | Validation shape: {Terraclimate_validation_df.shape}\")\nTerraclimate_validation_df.to_csv(\"terraclimate_features_validation.csv\", index=False)\ndisplay(Terraclimate_validation_df.head())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5f0fc36a-b4b7-44c4-bc59-7a1f6216d5ae",
      "metadata": {
        "name": "cell18",
        "language": "python"
      },
      "outputs": [],
      "source": "# Terraclimate_validation_df['Latitude'] = Validation_df['Latitude']\n# Terraclimate_validation_df['Longitude'] = Validation_df['Longitude']\n# Terraclimate_validation_df['Sample Date'] = Validation_df['Sample Date']\n# Terraclimate_validation_df = Terraclimate_validation_df[['Latitude', 'Longitude', 'Sample Date', 'pet']]\n# Terraclimate_validation_df.to_csv('terraclimate_features_validation.csv', index=False)\nTerraclimate_validation_df.to_csv(\"terraclimate_features_validation.csv\", index=False)\nprint(\"Saved:\", Terraclimate_validation_df.shape)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "23777c24-0347-4b35-ae06-f53ead075493",
      "metadata": {
        "name": "cell19",
        "language": "python"
      },
      "outputs": [],
      "source": "# Preview File\ndisplay(Terraclimate_validation_df.head())"
    },
    {
      "id": "7c8d9c1a-84d2-4f4b-9289-7850f2d80e5d",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "Terraclimate_validation_df.to_csv(\"/tmp/terraclimate_features_validation_sonnet_v2.csv\",index = False)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2e5b2806-b54b-4182-bf85-680f13edf181",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "session.sql(\"\"\"\n    PUT file:///tmp/terraclimate_features_validation_sonnet_v2.csv\n    'snow://workspace/USER$.PUBLIC.\"EY-AI-and-Data-Challenge\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\n\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d9938be-627b-44a5-8104-9d5bf2dcb07b",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "**Note:** If you're using your own workspace, remember to replace \"EY-AI-and-Data-Challenge\" with your workspace name in the file path."
    },
    {
      "id": "830b89f7-ed3e-4b09-8e89-c1b343cd4eeb",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "Terraclimate_validation_df",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ca9e2109-8df3-44b1-baa5-eb40079c0a92",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "Terraclimate_training_df",
      "outputs": [],
      "execution_count": null
    }
  ]
}